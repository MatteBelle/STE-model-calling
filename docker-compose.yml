version: "3.8"

services:
  llm-server:
    build:
      context: .
      dockerfile: Dockerfile.server
    container_name: llm-server
    runtime: nvidia
    environment:
      - HF_HOME=/huggingface_cache
      - HUGGING_FACE_HUB_TOKEN=<<YOUR_HUGGING_FACE_HUB_TOKEN>>
    volumes:
      - /home/belletti/huggingface_cache:/huggingface_cache
      - /home/belletti/STE-model-calling/STE:/home/belletti/STE-model-calling/STE
    ports:
      - "8000:8000"

  llm-main:
    build:
      context: .
      dockerfile: Dockerfile.main
    container_name: llm-main
    runtime: nvidia
    environment:
      - HF_HOME=/huggingface_cache
      - HUGGING_FACE_HUB_TOKEN=<<YOUR_HUGGING_FACE_HUB_TOKEN>>
      - MODEL_CKPT=meta-llama/Meta-Llama-3-8B-Instruct
      - NUM_EPISODES=3
      - NUM_STM_SLOTS=3
      - MAX_TURN=3
      - DIR_WRITE=STE/results/
      - RAPIDAPI_KEY=gaEUVyQ6X90IsxLekvznKw2ZJH5MFmOfWucC1p4loRDG8j3rqY
      - MODEL_SERVER_URL=http://llm-server:8000/generate
    volumes:
      - /home/belletti/STE-model-calling/STE:/home/belletti/STE-model-calling/STE
      - /home/belletti/huggingface_cache:/huggingface_cache
      - /home/belletti/STE-model-calling/STE/results:/home/belletti/STE-model-calling/STE/results
    depends_on:
      - llm-server