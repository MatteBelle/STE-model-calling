# Use the base image with CUDA support
FROM nvidia/cuda:12.3.2-devel-ubuntu20.04
LABEL maintainer="disi-unibo-nlp"

# Set noninteractive mode
ENV DEBIAN_FRONTEND=noninteractive

# Set working directory
WORKDIR /home/belletti/STE-model-calling/STE

# Install system dependencies and Python 3.9
RUN apt-get update -y && \
    apt-get install -y curl git bash nano wget python3.9 python3.9-distutils python3.9-venv python3-pip && \
    apt-get autoremove -y && \
    apt-get clean -y && \
    rm -rf /var/lib/apt/lists/*

# Make Python 3.9 the default python3
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1 && \
    update-alternatives --set python3 /usr/bin/python3.9

# Upgrade pip and install only the needed Python packages
RUN pip3 install --upgrade pip && \
    pip3 install --no-cache-dir fastapi uvicorn pydantic torch transformers accelerate==0.26.0

# Set the Hugging Face cache directory and create it
ENV HF_HOME="/huggingface_cache"
RUN mkdir -p /huggingface_cache

# Copy the llm_server.py file into the container (adjust the path as needed)
COPY STE/llm_server.py /home/belletti/STE-model-calling/STE/llm_server.py

# Expose the port that FastAPI will use
EXPOSE 8000

# Run the server using uvicorn. Note: using "llm_server:app" assumes that your file is named llm_server.py and that it defines "app".
CMD ["uvicorn", "llm_server:app", "--host", "0.0.0.0", "--port", "8000"]