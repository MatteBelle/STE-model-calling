[
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bertscore, rouge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bertscore\n  Description: BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.\n  Required parameters:\n    - predictions: The generated sentences.\n    - references: The reference sentences.\n    - lang: The language of the input text (e.g., 'en' for English).\n  Optional parameters:\n    - idf: Whether to use inverse document frequency reweighting (default: false).\n\nMetric name: rouge\n  Description: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.\n  Required parameters:\n    - predictions: list of *N* predictions to score. Each prediction should be a string with tokens separated by spaces\n    - references: list of *N* references (one for prediction) or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.\n  Optional parameters:\n    - rouge_types: Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).\n    - use_aggregator: If true, returns aggregated scores; if false, returns individual scores for each prediction-reference pair. Defaults to true.\n    - use_stemmer: If true, applies a Porter stemmer to normalize words before comparison. Defaults to false.\n\n\n\n### User Query: \nWhat are the ROUGE scores (rouge1, rouge2, rougeL, rougeLsum) with use_aggregator true and use_stemmer false, and the BERTScore with idf true for the following predictions and references: predictions = ['The sun is shining brightly in the clear blue sky', 'The birds are singing their sweet melodies'], references = ['The sun is shining brightly in the clear blue sky', 'The birds are singing their sweet melodies and filling the air with joy'].",
    "answer": "Action: rouge\nAction Input: {\n  \"predictions\": [\n    \"The sun is shining brightly in the clear blue sky\",\n    \"The birds are singing their sweet melodies\"\n  ],\n  \"references\": [\n    \"The sun is shining brightly in the clear blue sky\",\n    \"The birds are singing their sweet melodies and filling the air with joy\"\n  ],\n  \"rouge_types\": [\n    \"rouge1\",\n    \"rouge2\",\n    \"rougeL\",\n    \"rougeLsum\"\n  ],\n  \"use_aggregator\": \"true\",\n  \"use_stemmer\": \"false\"\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"The sun is shining brightly in the clear blue sky\",\n    \"The birds are singing their sweet melodies\"\n  ],\n  \"references\": [\n    \"The sun is shining brightly in the clear blue sky\",\n    \"The birds are singing their sweet melodies and filling the air with joy\"\n  ],\n  \"idf\": \"true\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bertscore, rouge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bertscore\n  Description: BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.\n  Required parameters:\n    - predictions: The generated sentences.\n    - references: The reference sentences.\n    - lang: The language of the input text (e.g., 'en' for English).\n  Optional parameters:\n    - idf: Whether to use inverse document frequency reweighting (default: false).\n\nMetric name: rouge\n  Description: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.\n  Required parameters:\n    - predictions: list of *N* predictions to score. Each prediction should be a string with tokens separated by spaces\n    - references: list of *N* references (one for prediction) or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.\n  Optional parameters:\n    - rouge_types: Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).\n    - use_aggregator: If true, returns aggregated scores; if false, returns individual scores for each prediction-reference pair. Defaults to true.\n    - use_stemmer: If true, applies a Porter stemmer to normalize words before comparison. Defaults to false.\n\n\n\n### User Query: \nWhat are the ROUGE and BERTScore of the following pairs of texts: predictions=['The new car has a sleek design', 'The new car is very expensive']; references=[['The new car has a sleek design and a powerful engine', 'The new car is very expensive and has a long warranty'], ['The new car has a good design but a small engine', 'The new car is not very expensive']]; rouge_types=['rouge1', 'rougeLsum']; idf=true; lang=en?",
    "answer": "Action: rouge\nAction Input: {\n  \"predictions\": [\n    \"The new car has a sleek design\",\n    \"The new car is very expensive\"\n  ],\n  \"references\": [\n    [\n      \"The new car has a sleek design and a powerful engine\",\n      \"The new car is very expensive and has a long warranty\"\n    ],\n    [\n      \"The new car has a good design but a small engine\",\n      \"The new car is not very expensive\"\n    ]\n  ],\n  \"rouge_types\": [\n    \"rouge1\",\n    \"rougeLsum\"\n  ]\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"The new car has a sleek design\",\n    \"The new car is very expensive\"\n  ],\n  \"references\": [\n    [\n      \"The new car has a sleek design and a powerful engine\",\n      \"The new car is very expensive and has a long warranty\"\n    ],\n    [\n      \"The new car has a good design but a small engine\",\n      \"The new car is not very expensive\"\n    ]\n  ],\n  \"idf\": true,\n  \"lang\": \"en\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bertscore, bleu, rouge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bertscore\n  Description: BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.\n  Required parameters:\n    - predictions: The generated sentences.\n    - references: The reference sentences.\n    - lang: The language of the input text (e.g., 'en' for English).\n  Optional parameters:\n    - idf: Whether to use inverse document frequency reweighting (default: false).\n\nMetric name: bleu\n  Description: BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of *N* machine translation by comparing a candidate translation against *N* reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.\n  Required parameters:\n    - predictions: List of *N* translations to score\n    - references: list of *N* reference translations or a list of *N* lists of reference translations.\n  Optional parameters:\n    - max_order: Maximum n-gram order to consider (default: 4).\n    - smooth: Whether to apply smoothing (default: false).\n\nMetric name: rouge\n  Description: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.\n  Required parameters:\n    - predictions: list of *N* predictions to score. Each prediction should be a string with tokens separated by spaces\n    - references: list of *N* references (one for prediction) or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.\n  Optional parameters:\n    - rouge_types: Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).\n    - use_aggregator: If true, returns aggregated scores; if false, returns individual scores for each prediction-reference pair. Defaults to true.\n    - use_stemmer: If true, applies a Porter stemmer to normalize words before comparison. Defaults to false.\n\n\n\n### User Query: \nWhat are the ROUGE, BLEU, and BERTScore for these predictions: ['the sun is shining in the sky with beautiful clouds', 'the sun is shining brightly in the morning'], references=[['the sun is shining in the sky with beautiful white clouds', 'the sun is shining brightly in the morning and it is warm'], ['the sun is shining in the sky with big fluffy clouds', 'the sun is shining brightly in the morning and the birds are singing']], rouge_types=['rouge1', 'rougeL'], use_aggregator=True, use_stemmer=False, max_order=3, smooth=True, idf=True, lang='en'?",
    "answer": "Action: rouge\nAction Input: {\n  \"predictions\": [\n    \"the sun is shining in the sky with beautiful clouds\",\n    \"the sun is shining brightly in the morning\"\n  ],\n  \"references\": [\n    [\n      \"the sun is shining in the sky with beautiful white clouds\",\n      \"the sun is shining brightly in the morning and it is warm\"\n    ],\n    [\n      \"the sun is shining in the sky with big fluffy clouds\",\n      \"the sun is shining brightly in the morning and the birds are singing\"\n    ]\n  ],\n  \"rouge_types\": [\n    \"rouge1\",\n    \"rougeL\"\n  ]\n}\n\nAction: bleu\nAction Input: {\n  \"predictions\": [\n    \"the sun is shining in the sky with beautiful clouds\",\n    \"the sun is shining brightly in the morning\"\n  ],\n  \"references\": [\n    [\n      \"the sun is shining in the sky with beautiful white clouds\",\n      \"the sun is shining brightly in the morning and it is warm\"\n    ],\n    [\n      \"the sun is shining in the sky with big fluffy clouds\",\n      \"the sun is shining brightly in the morning and the birds are singing\"\n    ]\n  ],\n  \"max_order\": 3,\n  \"smooth\": true\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"the sun is shining in the sky with beautiful clouds\",\n    \"the sun is shining brightly in the morning\"\n  ],\n  \"references\": [\n    [\n      \"the sun is shining in the sky with beautiful white clouds\",\n      \"the sun is shining brightly in the morning and it is warm\"\n    ],\n    [\n      \"the sun is shining in the sky with big fluffy clouds\",\n      \"the sun is shining brightly in the morning and the birds are singing\"\n    ]\n  ],\n  \"lang\": \"en\",\n  \"idf\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bertscore, bleu, rouge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bertscore\n  Description: BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.\n  Required parameters:\n    - predictions: The generated sentences.\n    - references: The reference sentences.\n    - lang: The language of the input text (e.g., 'en' for English).\n  Optional parameters:\n    - idf: Whether to use inverse document frequency reweighting (default: false).\n\nMetric name: bleu\n  Description: BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of *N* machine translation by comparing a candidate translation against *N* reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.\n  Required parameters:\n    - predictions: List of *N* translations to score\n    - references: list of *N* reference translations or a list of *N* lists of reference translations.\n  Optional parameters:\n    - max_order: Maximum n-gram order to consider (default: 4).\n    - smooth: Whether to apply smoothing (default: false).\n\nMetric name: rouge\n  Description: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.\n  Required parameters:\n    - predictions: list of *N* predictions to score. Each prediction should be a string with tokens separated by spaces\n    - references: list of *N* references (one for prediction) or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.\n  Optional parameters:\n    - rouge_types: Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).\n    - use_aggregator: If true, returns aggregated scores; if false, returns individual scores for each prediction-reference pair. Defaults to true.\n    - use_stemmer: If true, applies a Porter stemmer to normalize words before comparison. Defaults to false.\n\n\n\n### User Query: \nWhat are the BLEU scores with max_order 4 and smooth false, the ROUGE scores (rouge1, rouge2, rougeL, rougeLsum) with use_aggregator true and use_stemmer false, and the BERTScore with idf false for the following predictions and references: predictions = ['The cat is sleeping on the mat', 'The dog is running quickly'], references = [['The cat is sleeping on the mat', 'The cat is sleeping on the mat'], ['The dog is running quickly', 'The dog is running quickly']].",
    "answer": "Action: bleu\nAction Input: {\n  \"predictions\": [\n    \"The cat is sleeping on the mat\",\n    \"The dog is running quickly\"\n  ],\n  \"references\": [\n    [\n      \"The cat is sleeping on the mat\",\n      \"The cat is sleeping on the mat\"\n    ],\n    [\n      \"The dog is running quickly\",\n      \"The dog is running quickly\"\n    ]\n  ],\n  \"max_order\": 4,\n  \"smooth\": false\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"The cat is sleeping on the mat\",\n    \"The dog is running quickly\"\n  ],\n  \"references\": [\n    [\n      \"The cat is sleeping on the mat\",\n      \"The cat is sleeping on the mat\"\n    ],\n    [\n      \"The dog is running quickly\",\n      \"The dog is running quickly\"\n    ]\n  ],\n  \"lang\": \"en\",\n  \"idf\": false\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: llm_judge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: llm_judge\n  Description: LLM Judge uses a language model's own evaluation capabilities to assess the quality of a candidate text from multiple perspectives. Given a candidate text, a set of quality criteria, and optionally reference texts, the model returns a score (on a defined scale) and, if requested, a detailed explanation of its judgment.\n  Required parameters:\n    - candidate_texts: The texts whose quality is to be evaluated.\n    - quality_criteria: A list of aspects to evaluate (coherence, creativity, relevance, fluency).\n    - scale_max: The maximum value on the evaluation scale (e.g., 10 means scores will be in the range 0-10).\n  Optional parameters:\n    - explanation_required: If true, the LLM should provide a textual explanation along with the score.\n    - evaluation_type: The type of evaluation to perform (e.g., 'numeric', 'binary', 'qualitative'). Defaults to 'numeric'.\n    - prompt_template: A custom prompt template to guide the LLM's evaluation process, if desired.\n\n\n\n### User Query: \nWhat are the scores and explanations for evaluating the quality of these six creative texts: \"The smell of freshly cut grass is a classic summer scent\", \"The sound of laughter is contagious and uplifting\", \"A beautiful sunset on a tranquil lake\", \"The feeling of sand between my toes is a summer delight\", \"A warm cup of coffee on a chilly morning is invigorating\", and \"The sound of children's laughter is contagious and uplifting\", using the criteria of coherence, creativity, and relevance, on a scale of 10, with detailed explanation, using a binary evaluation type, and a custom prompt template \"Please evaluate the texts based on their ability to evoke a sense of joy and relaxation\"?",
    "answer": "Action: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"The smell of freshly cut grass is a classic summer scent\",\n    \"The sound of laughter is contagious and uplifting\",\n    \"A beautiful sunset on a tranquil lake\",\n    \"The feeling of sand between my toes is a summer delight\",\n    \"A warm cup of coffee on a chilly morning is invigorating\",\n    \"The sound of children's laughter is contagious and uplifting\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": false,\n  \"evaluation_type\": \"numeric\",\n  \"prompt_template\": \"Please evaluate the overall quality of the texts\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: llm_judge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: llm_judge\n  Description: LLM Judge uses a language model's own evaluation capabilities to assess the quality of a candidate text from multiple perspectives. Given a candidate text, a set of quality criteria, and optionally reference texts, the model returns a score (on a defined scale) and, if requested, a detailed explanation of its judgment.\n  Required parameters:\n    - candidate_texts: The texts whose quality is to be evaluated.\n    - quality_criteria: A list of aspects to evaluate (coherence, creativity, relevance, fluency).\n    - scale_max: The maximum value on the evaluation scale (e.g., 10 means scores will be in the range 0-10).\n  Optional parameters:\n    - explanation_required: If true, the LLM should provide a textual explanation along with the score.\n    - evaluation_type: The type of evaluation to perform (e.g., 'numeric', 'binary', 'qualitative'). Defaults to 'numeric'.\n    - prompt_template: A custom prompt template to guide the LLM's evaluation process, if desired.\n\n\n\n### User Query: \nWhat are the scores and explanations for evaluating the quality of these four creative texts: \"The smell of freshly baked cookies wafts through the house\", \"A quiet evening by the fireplace is the perfect way to unwind\", \"The feeling of a cool breeze on a hot summer day is invigorating\", \"The sound of a babbling brook is soothing\", using the criteria of coherence, creativity, and relevance, on a scale of 10, with detailed explanation, using a binary evaluation type, and a custom prompt template \"Please evaluate the texts based on their ability to evoke a sense of relaxation and contentment\"?",
    "answer": "Action: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"The smell of freshly baked cookies wafts through the house\",\n    \"A quiet evening by the fireplace is the perfect way to unwind\",\n    \"The feeling of a cool breeze on a hot summer day is invigorating\",\n    \"The sound of a babbling brook is soothing\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": true,\n  \"evaluation_type\": \"binary\",\n  \"prompt_template\": \"Please evaluate the texts based on their ability to evoke a sense of relaxation and contentment\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: llm_judge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: llm_judge\n  Description: LLM Judge uses a language model's own evaluation capabilities to assess the quality of a candidate text from multiple perspectives. Given a candidate text, a set of quality criteria, and optionally reference texts, the model returns a score (on a defined scale) and, if requested, a detailed explanation of its judgment.\n  Required parameters:\n    - candidate_texts: The texts whose quality is to be evaluated.\n    - quality_criteria: A list of aspects to evaluate (coherence, creativity, relevance, fluency).\n    - scale_max: The maximum value on the evaluation scale (e.g., 10 means scores will be in the range 0-10).\n  Optional parameters:\n    - explanation_required: If true, the LLM should provide a textual explanation along with the score.\n    - evaluation_type: The type of evaluation to perform (e.g., 'numeric', 'binary', 'qualitative'). Defaults to 'numeric'.\n    - prompt_template: A custom prompt template to guide the LLM's evaluation process, if desired.\n\n\n\n### User Query: \nWhat are the scores for the following creative texts: \"The morning dew glistened on the grass.\", \"The sound of the birds chirping filled the air.\", \"The warmth of the sun on my face was invigorating.\" according to the criteria coherence, creativity, and relevance on a scale of 10, with a detailed explanation, a binary evaluation, using the prompt template \"Please evaluate the quality of the text as a passage from a spring morning\", and with the evaluation type set to \"numeric\", and also compare them to the scores of the texts: \"The quality of the text is not good\", \"The quality of the text is not good\", \"The quality of the text is not good\"?",
    "answer": "Action: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"The morning dew glistened on the grass.\",\n    \"The sound of the birds chirping filled the air.\",\n    \"The warmth of the sun on my face was invigorating.\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": true,\n  \"evaluation_type\": \"binary\",\n  \"prompt_template\": \"Please evaluate the quality of the text as a passage from a spring morning\"\n}\n\nAction: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"The quality of the text is not good\",\n    \"The quality of the text is not good\",\n    \"The quality of the text is not good\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": true,\n  \"evaluation_type\": \"binary\",\n  \"prompt_template\": \"Please evaluate the quality of the text as a passage from a spring morning\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: llm_judge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: llm_judge\n  Description: LLM Judge uses a language model's own evaluation capabilities to assess the quality of a candidate text from multiple perspectives. Given a candidate text, a set of quality criteria, and optionally reference texts, the model returns a score (on a defined scale) and, if requested, a detailed explanation of its judgment.\n  Required parameters:\n    - candidate_texts: The texts whose quality is to be evaluated.\n    - quality_criteria: A list of aspects to evaluate (coherence, creativity, relevance, fluency).\n    - scale_max: The maximum value on the evaluation scale (e.g., 10 means scores will be in the range 0-10).\n  Optional parameters:\n    - explanation_required: If true, the LLM should provide a textual explanation along with the score.\n    - evaluation_type: The type of evaluation to perform (e.g., 'numeric', 'binary', 'qualitative'). Defaults to 'numeric'.\n    - prompt_template: A custom prompt template to guide the LLM's evaluation process, if desired.\n\n\n\n### User Query: \nWhat are the scores for evaluating the texts \"A world where AI and humans collaborate seamlessly\", \"The AI-generated art piece was a masterpiece with multiple layers of meaning\", and \"The robot's AI system was surprisingly empathetic\" using the quality criteria \"coherence\", \"creativity\", and \"relevance\" with a scale max of 10?",
    "answer": "Action: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"A world where AI and humans collaborate seamlessly\",\n    \"The AI-generated art piece was a masterpiece with multiple layers of meaning\",\n    \"The robot's AI  was surprisingly empathetic\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": \"10\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: llm_judge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: llm_judge\n  Description: LLM Judge uses a language model's own evaluation capabilities to assess the quality of a candidate text from multiple perspectives. Given a candidate text, a set of quality criteria, and optionally reference texts, the model returns a score (on a defined scale) and, if requested, a detailed explanation of its judgment.\n  Required parameters:\n    - candidate_texts: The texts whose quality is to be evaluated.\n    - quality_criteria: A list of aspects to evaluate (coherence, creativity, relevance, fluency).\n    - scale_max: The maximum value on the evaluation scale (e.g., 10 means scores will be in the range 0-10).\n  Optional parameters:\n    - explanation_required: If true, the LLM should provide a textual explanation along with the score.\n    - evaluation_type: The type of evaluation to perform (e.g., 'numeric', 'binary', 'qualitative'). Defaults to 'numeric'.\n    - prompt_template: A custom prompt template to guide the LLM's evaluation process, if desired.\n\n\n\n### User Query: \nWhat are the perplexities of the following group of texts: \"The sun sets slowly over the ocean.\", \"The sky turns pink and orange.\", \"The world is full of tranquility.\", \"The seagulls fly overhead.\", \"The smell of saltwater fills the air.\" and \"The stars shine brightly in the evening sky.\", \"The world is full of wonder.\", \"The flowers bloom in every color.\", \"The baby laughs at the bubbles.\", \"The kids play with the ball.\", \"The world is full of joy.\" using the gpt2 model?",
    "answer": "Action: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The sun sets slowly over the ocean.\",\n    \"The sky turns pink and orange.\",\n    \"The world is full of tranquility.\",\n    \"The seagulls fly overhead.\",\n    \"The smell of saltwater fills the air.\",\n    \"The stars shine brightly in the evening sky.\",\n    \"The world is full of wonder.\",\n    \"The flowers bloom in every color.\",\n    \"The baby laughs at the bubbles.\",\n    \"The kids play with the ball.\",\n    \"The world is full of joy.\"\n  ],\n  \"model_id\": \"gpt2\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: llm_judge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: llm_judge\n  Description: LLM Judge uses a language model's own evaluation capabilities to assess the quality of a candidate text from multiple perspectives. Given a candidate text, a set of quality criteria, and optionally reference texts, the model returns a score (on a defined scale) and, if requested, a detailed explanation of its judgment.\n  Required parameters:\n    - candidate_texts: The texts whose quality is to be evaluated.\n    - quality_criteria: A list of aspects to evaluate (coherence, creativity, relevance, fluency).\n    - scale_max: The maximum value on the evaluation scale (e.g., 10 means scores will be in the range 0-10).\n  Optional parameters:\n    - explanation_required: If true, the LLM should provide a textual explanation along with the score.\n    - evaluation_type: The type of evaluation to perform (e.g., 'numeric', 'binary', 'qualitative'). Defaults to 'numeric'.\n    - prompt_template: A custom prompt template to guide the LLM's evaluation process, if desired.\n\n\n\n### User Query: \nWhat are the scores and explanations for evaluating the texts \"The AI-generated novel was a thrilling adventure with a surprising plot\", \"The robot's AI assistant was incredibly helpful and efficient\", and \"A futuristic city's AI-powered transportation system was surprisingly convenient\" using the quality criteria \"coherence\", \"creativity\", and \"relevance\" with a scale max of 10, explanation required, evaluation type as \"numeric\", and prompt template \"Please evaluate the text's ability to transport the reader to a new and exciting world\"?",
    "answer": "Action: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"The AI-generated novel was a thrilling adventure with a surprising plot\",\n    \"The robot's AI  was incredibly helpful and efficient\",\n    \"A futuristic city's AI-powered transportation  was surprisingly convenient\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": true,\n  \"evaluation_type\": \"numeric\",\n  \"prompt_template\": \"Please evaluate the text's ability to transport the reader to a new and exciting world\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: llm_judge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: llm_judge\n  Description: LLM Judge uses a language model's own evaluation capabilities to assess the quality of a candidate text from multiple perspectives. Given a candidate text, a set of quality criteria, and optionally reference texts, the model returns a score (on a defined scale) and, if requested, a detailed explanation of its judgment.\n  Required parameters:\n    - candidate_texts: The texts whose quality is to be evaluated.\n    - quality_criteria: A list of aspects to evaluate (coherence, creativity, relevance, fluency).\n    - scale_max: The maximum value on the evaluation scale (e.g., 10 means scores will be in the range 0-10).\n  Optional parameters:\n    - explanation_required: If true, the LLM should provide a textual explanation along with the score.\n    - evaluation_type: The type of evaluation to perform (e.g., 'numeric', 'binary', 'qualitative'). Defaults to 'numeric'.\n    - prompt_template: A custom prompt template to guide the LLM's evaluation process, if desired.\n\n\n\n### User Query: \nHow do the candidate texts \"The movie was a game-changer with its stunning visuals and unexpected plot twists\" and \"The plot was engaging, but the visuals were lacking\" compare to the reference texts \"The Lord of the Rings\" and \"Inception\" in terms of coherence, creativity, and fluency, with a scale of 10, a numeric evaluation type, and the prompt template \"Please evaluate the candidate texts based on their ability to tell a compelling story\", and provide a detailed explanation for the scores?",
    "answer": "Action: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"The movie was a game-changer with its stunning visuals and unexpected plot twists\",\n    \"The plot was engaging, but the visuals were lacking\"\n  ],\n  \"quality_criteria\": [\n    \"overall_quality\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": true,\n  \"evaluation_type\": \"numeric\",\n  \"prompt_template\": \"Please evaluate the candidate texts based on their overall quality\",\n  \"reference_texts\": [\n    \"The Lord of the Rings\",\n    \"Inception\"\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: llm_judge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: llm_judge\n  Description: LLM Judge uses a language model's own evaluation capabilities to assess the quality of a candidate text from multiple perspectives. Given a candidate text, a set of quality criteria, and optionally reference texts, the model returns a score (on a defined scale) and, if requested, a detailed explanation of its judgment.\n  Required parameters:\n    - candidate_texts: The texts whose quality is to be evaluated.\n    - quality_criteria: A list of aspects to evaluate (coherence, creativity, relevance, fluency).\n    - scale_max: The maximum value on the evaluation scale (e.g., 10 means scores will be in the range 0-10).\n  Optional parameters:\n    - explanation_required: If true, the LLM should provide a textual explanation along with the score.\n    - evaluation_type: The type of evaluation to perform (e.g., 'numeric', 'binary', 'qualitative'). Defaults to 'numeric'.\n    - prompt_template: A custom prompt template to guide the LLM's evaluation process, if desired.\n\n\n\n### User Query: \nWhat are the scores and explanations for evaluating the quality of these three creative texts: \"The sound of ocean waves crashing against the shore is mesmerizing\", \"A warm cup of hot chocolate on a cold winter morning is comforting\", \"The feeling of a gentle breeze on a spring morning is refreshing\", using the criteria of coherence, creativity, and relevance, on a scale of 10, with detailed explanation, using a binary evaluation type, and a custom prompt template \"Please assess the texts based on their ability to evoke a sense of calmness and coziness\"?",
    "answer": "Action: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"The sound of ocean waves crashing against the shore is mesmerizing\",\n    \"A warm cup of hot chocolate on a cold winter morning is comforting\",\n    \"The feeling of a gentle breeze on a spring morning is refreshing\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": true,\n  \"evaluation_type\": \"binary\",\n  \"prompt_template\": \"Please assess the texts based on their ability to evoke a sense of calmness and coziness\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: llm_judge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: llm_judge\n  Description: LLM Judge uses a language model's own evaluation capabilities to assess the quality of a candidate text from multiple perspectives. Given a candidate text, a set of quality criteria, and optionally reference texts, the model returns a score (on a defined scale) and, if requested, a detailed explanation of its judgment.\n  Required parameters:\n    - candidate_texts: The texts whose quality is to be evaluated.\n    - quality_criteria: A list of aspects to evaluate (coherence, creativity, relevance, fluency).\n    - scale_max: The maximum value on the evaluation scale (e.g., 10 means scores will be in the range 0-10).\n  Optional parameters:\n    - explanation_required: If true, the LLM should provide a textual explanation along with the score.\n    - evaluation_type: The type of evaluation to perform (e.g., 'numeric', 'binary', 'qualitative'). Defaults to 'numeric'.\n    - prompt_template: A custom prompt template to guide the LLM's evaluation process, if desired.\n\n\n\n### User Query: \nWhat are the scores and explanations for evaluating the quality of these five creative texts: \"The smell of freshly baked cookies wafts through the house\", \"A quiet evening by the fireplace is the perfect way to unwind\", \"The feeling of a cool breeze on a hot summer day is invigorating\", \"The sound of a babbling brook is soothing\", and \"A warm cup of tea on a chilly morning is comforting\", using the criteria of coherence, creativity, and relevance, on a scale of 10, with detailed explanation, using a binary evaluation type, and a custom prompt template \"Please evaluate the texts based on their ability to evoke a sense of relaxation and contentment\"?",
    "answer": "Action: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"The smell of freshly baked cookies wafts through the house\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": true,\n  \"evaluation_type\": \"binary\",\n  \"prompt_template\": \"Please evaluate the text based on its ability to evoke a sense of relaxation and contentment\"\n}\n\nAction: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"A quiet evening by the fireplace is the perfect way to unwind\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": true,\n  \"evaluation_type\": \"binary\",\n  \"prompt_template\": \"Please evaluate the text based on its ability to evoke a sense of relaxation and contentment\"\n}\n\nAction: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"The feeling of a cool breeze on a hot summer day is invigorating\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": true,\n  \"evaluation_type\": \"binary\",\n  \"prompt_template\": \"Please evaluate the text based on its ability to evoke a sense of relaxation and contentment\"\n}\n\nAction: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"The sound of a babbling brook is soothing\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": true,\n  \"evaluation_type\": \"binary\",\n  \"prompt_template\": \"Please evaluate the text based on its ability to evoke a sense of relaxation and contentment\"\n}\n\nAction: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"A warm cup of tea on a chilly morning is comforting\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": true,\n  \"evaluation_type\": \"binary\",\n  \"prompt_template\": \"Please evaluate the text based on its ability to evoke a sense of relaxation and contentment\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: llm_judge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: llm_judge\n  Description: LLM Judge uses a language model's own evaluation capabilities to assess the quality of a candidate text from multiple perspectives. Given a candidate text, a set of quality criteria, and optionally reference texts, the model returns a score (on a defined scale) and, if requested, a detailed explanation of its judgment.\n  Required parameters:\n    - candidate_texts: The texts whose quality is to be evaluated.\n    - quality_criteria: A list of aspects to evaluate (coherence, creativity, relevance, fluency).\n    - scale_max: The maximum value on the evaluation scale (e.g., 10 means scores will be in the range 0-10).\n  Optional parameters:\n    - explanation_required: If true, the LLM should provide a textual explanation along with the score.\n    - evaluation_type: The type of evaluation to perform (e.g., 'numeric', 'binary', 'qualitative'). Defaults to 'numeric'.\n    - prompt_template: A custom prompt template to guide the LLM's evaluation process, if desired.\n\n\n\n### User Query: \nWhat are the scores and explanations for evaluating the quality of these three creative texts: \"The stars shone brightly in the night sky\", \"A gentle summer breeze rustled the leaves of the trees\", \"The soft chirping of birds filled the morning air\", using the criteria of coherence, creativity, and relevance, on a scale of 10, with detailed explanation, using a binary evaluation type, and a custom prompt template \"Please evaluate the texts based on their ability to evoke a sense of wonder and awe, while also considering their ability to create a sense of atmosphere and mood\"?",
    "answer": "Action: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"The stars shone brightly in the night sky\",\n    \"A gentle summer breeze rustled the leaves of the trees\",\n    \"The soft chirping of birds filled the morning air\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": true,\n  \"evaluation_type\": \"binary\",\n  \"prompt_template\": \"Please evaluate the texts based on their ability to evoke a sense of wonder and awe, while also considering their ability to create a sense of atmosphere and mood\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: accuracy, f1\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: accuracy\n  Description: Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.\n  Required parameters:\n    - predictions: Predicted labels\n    - references: Ground truth labels.\n  Optional parameters:\n    - normalize: If set to false, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples\n    - sample_weight: Predicted labels\n\nMetric name: f1\n  Description: The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.\n  Required parameters:\n    - predictions: The *N* predicted labels.\n    - references: The *N* true labels.\n  Optional parameters:\n    - average: The averaging method to use (e.g., 'macro', 'micro', 'weighted').\n\n\n\n### User Query: \nWhat are the F1 scores for a multi-class classification model when the predicted labels are [2, 1, 0] and the ground truth labels are [2, 0, 1], and how does the accuracy of the model change when the predicted labels are [2, 2, 0] and the ground truth labels are [2, 0, 1] with sample weights of [0.6, 0.3, 0.1] compared to the case where the sample weights are [0.7, 0.2, 0.1]?",
    "answer": "Action: f1\nAction Input: {\n  \"predictions\": [\n    2,\n    1,\n    0\n  ],\n  \"references\": [\n    2,\n    0,\n    1\n  ],\n  \"average\": \"macro\"\n}\n\nAction: f1\nAction Input: {\n  \"predictions\": [\n    2,\n    2,\n    0\n  ],\n  \"references\": [\n    2,\n    0,\n    1\n  ],\n  \"average\": \"macro\"\n}\n\nAction: accuracy\nAction Input: {\n  \"predictions\": [\n    2,\n    2,\n    0\n  ],\n  \"references\": [\n    2,\n    0,\n    1\n  ],\n  \"normalize\": true,\n  \"sample_weight\": [\n    0.6,\n    0.3,\n    0.1\n  ]\n}\n\nAction: accuracy\nAction Input: {\n  \"predictions\": [\n    2,\n    2,\n    0\n  ],\n  \"references\": [\n    2,\n    0,\n    1\n  ],\n  \"normalize\": true,\n  \"sample_weight\": [\n    0.7,\n    0.2,\n    0.1\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: accuracy\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: accuracy\n  Description: Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.\n  Required parameters:\n    - predictions: Predicted labels\n    - references: Ground truth labels.\n  Optional parameters:\n    - normalize: If set to false, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples\n    - sample_weight: Predicted labels\n\n\n\n### User Query: \nWhat is the accuracy of a model that correctly classifies 5 out of 10 samples with labels [1, 0, 1, 1, 0, 0, 1, 1, 0, 0] and the ground truth labels are [1, 0, 1, 1, 0, 0, 1, 0, 1, 0]?",
    "answer": "Action: accuracy\nAction Input: {\n  \"predictions\": [\n    1,\n    0,\n    1,\n    1,\n    0,\n    0,\n    1,\n    1,\n    0,\n    0\n  ],\n  \"references\": [\n    1,\n    0,\n    1,\n    1,\n    0,\n    0,\n    1,\n    0,\n    1,\n    0\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: accuracy, f1\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: accuracy\n  Description: Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.\n  Required parameters:\n    - predictions: Predicted labels\n    - references: Ground truth labels.\n  Optional parameters:\n    - normalize: If set to false, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples\n    - sample_weight: Predicted labels\n\nMetric name: f1\n  Description: The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.\n  Required parameters:\n    - predictions: The *N* predicted labels.\n    - references: The *N* true labels.\n  Optional parameters:\n    - average: The averaging method to use (e.g., 'macro', 'micro', 'weighted').\n\n\n\n### User Query: \nWhat are the accuracy and F1 scores for the following predictions and references: predictions=[0, 1, 1, 0], references=[0, 0, 1, 1], normalize=false, sample_weight=[0.7, 0.2, 0.05, 0.05], average='macro'?",
    "answer": "Action: accuracy\nAction Input: {\n  \"predictions\": [\n    0,\n    1,\n    1,\n    0\n  ],\n  \"references\": [\n    0,\n    0,\n    1,\n    1\n  ],\n  \"normalize\": false,\n  \"sample_weight\": [\n    0.7,\n    0.2,\n    0.05,\n    0.05\n  ]\n}\n\nAction: f1\nAction Input: {\n  \"predictions\": [\n    0,\n    1,\n    1,\n    0\n  ],\n  \"references\": [\n    0,\n    0,\n    1,\n    1\n  ],\n  \"average\": \"macro\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: accuracy\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: accuracy\n  Description: Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.\n  Required parameters:\n    - predictions: Predicted labels\n    - references: Ground truth labels.\n  Optional parameters:\n    - normalize: If set to false, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples\n    - sample_weight: Predicted labels\n\n\n\n### User Query: \nWhat is the accuracy of my model when I have predicted labels [1, 1, 1, 0, 0, 1] and the actual labels are [1, 1, 0, 0, 1, 1] with sample weights 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, and I want the accuracy to be a fraction?",
    "answer": "Action: accuracy\nAction Input: {\n  \"predictions\": [\n    1,\n    1,\n    1,\n    0,\n    0,\n    1\n  ],\n  \"references\": [\n    1,\n    1,\n    0,\n    0,\n    1,\n    1\n  ],\n  \"sample_weight\": [\n    0.2,\n    0.3,\n    0.4,\n    0.5,\n    0.6,\n    0.7\n  ],\n  \"normalize\": \"true\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: accuracy\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: accuracy\n  Description: Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.\n  Required parameters:\n    - predictions: Predicted labels\n    - references: Ground truth labels.\n  Optional parameters:\n    - normalize: If set to false, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples\n    - sample_weight: Predicted labels\n\n\n\n### User Query: \nWhat is the accuracy when I have predicted labels [1, 1, 0, 1, 0, 1] and the actual labels are [1, 1, 0, 0, 0, 1], I want the proportion of correct predictions, and the samples have weights [0.5, 0.2, 0.8, 0.4, 0.6, 0.9]?",
    "answer": "Action: accuracy\nAction Input: {\n  \"predictions\": [\n    1,\n    1,\n    0,\n    1,\n    0,\n    1\n  ],\n  \"references\": [\n    1,\n    1,\n    0,\n    0,\n    0,\n    1\n  ],\n  \"normalize\": true,\n  \"sample_weight\": [\n    0.5,\n    0.2,\n    0.8,\n    0.4,\n    0.6,\n    0.9\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: accuracy\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: accuracy\n  Description: Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.\n  Required parameters:\n    - predictions: Predicted labels\n    - references: Ground truth labels.\n  Optional parameters:\n    - normalize: If set to false, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples\n    - sample_weight: Predicted labels\n\n\n\n### User Query: \nWhat is the accuracy of the model when it predicts [0, 1, 1, 0, 1] and the actual labels are [1, 1, 0, 1, 0] with normalize set to False and sample_weight set to [0.4, 0.3, 0.1, 0.05, 0.15]?",
    "answer": "Action: accuracy\nAction Input: {\n  \"predictions\": [\n    0,\n    1,\n    1,\n    0,\n    1\n  ],\n  \"references\": [\n    1,\n    1,\n    0,\n    1,\n    0\n  ],\n  \"normalize\": false,\n  \"sample_weight\": [\n    0.4,\n    0.3,\n    0.1,\n    0.05,\n    0.15\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: accuracy\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: accuracy\n  Description: Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.\n  Required parameters:\n    - predictions: Predicted labels\n    - references: Ground truth labels.\n  Optional parameters:\n    - normalize: If set to false, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples\n    - sample_weight: Predicted labels\n\n\n\n### User Query: \nWhat is the model's accuracy when predictions are [0, 1, 1, 0, 1, 0] and references are [1, 1, 0, 0, 1, 0] with normalize set to False and sample_weight set to [0.5, 0.3, 0.1, 0.05, 0.2, 0.15]?",
    "answer": "Action: accuracy\nAction Input: {\n  \"predictions\": [\n    0,\n    1,\n    1,\n    0,\n    1,\n    0\n  ],\n  \"references\": [\n    1,\n    1,\n    0,\n    0,\n    1,\n    0\n  ],\n  \"normalize\": false,\n  \"sample_weight\": [\n    0.5,\n    0.3,\n    0.1,\n    0.05,\n    0.2,\n    0.15\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: accuracy\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: accuracy\n  Description: Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.\n  Required parameters:\n    - predictions: Predicted labels\n    - references: Ground truth labels.\n  Optional parameters:\n    - normalize: If set to false, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples\n    - sample_weight: Predicted labels\n\n\n\n### User Query: \nWhat is the accuracy of the model when the predicted labels are [1, 0, 1, 1, 0] and the ground truth labels are [1, 1, 1, 0, 0], with normalization enabled and sample weights of [0.5, 0.3, 0.2, 0.1, 0.9]?",
    "answer": "Action: accuracy\nAction Input: {\n  \"predictions\": [\n    1,\n    0,\n    1,\n    1,\n    0\n  ],\n  \"references\": [\n    1,\n    1,\n    1,\n    0,\n    0\n  ],\n  \"normalize\": true,\n  \"sample_weight\": [\n    0.5,\n    0.3,\n    0.2,\n    0.1,\n    0.9\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: exact_match\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: exact_match\n  Description: Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.\n  Required parameters:\n    - predictions: List of predicted texts.\n    - references: List of reference texts.\n  Optional parameters:\n    - regexes_to_ignore: Regex expressions of characters to ignore when calculating the exact matches.\n    - ignore_case: If true, turns everything to lowercase so that capitalization differences are ignored.\n    - ignore_numbers (bool): If true, removes all digits before comparing strings\n    - ignore_punctuation (bool): If true, removes punctuation before comparing strings.\n\n\n\n### User Query: \nWhat are the exact match scores for the predictions ['New York', 'Chicago', 'Los Angeles', 'Houston', 'Philadelphia', 'Phoenix'] against references ['New York', 'Chicago', 'Los Angeles', 'Houston', 'Philadelphia', 'San Antonio'] and what are the exact match scores for the predictions ['New York', 'Chicago', 'Los Angeles', 'Houston', 'Philadelphia', 'Phoenix'] against references ['New York', 'Chicago', 'Los Angeles', 'Houston', 'Philadelphia', 'Chicago']?",
    "answer": "Action: exact_match\nAction Input: {\n  \"predictions\": [\n    \"New York\",\n    \"Chicago\",\n    \"Los Angeles\",\n    \"Houston\",\n    \"Philadelphia\",\n    \"Phoenix\"\n  ],\n  \"references\": [\n    \"New York\",\n    \"Chicago\",\n    \"Los Angeles\",\n    \"Houston\",\n    \"Philadelphia\",\n    \"San Antonio\"\n  ]\n}\n\nAction: exact_match\nAction Input: {\n  \"predictions\": [\n    \"New York\",\n    \"Chicago\",\n    \"Los Angeles\",\n    \"Houston\",\n    \"Philadelphia\",\n    \"Phoenix\"\n  ],\n  \"references\": [\n    \"New York\",\n    \"Chicago\",\n    \"Los Angeles\",\n    \"Houston\",\n    \"Philadelphia\",\n    \"Chicago\"\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: exact_match\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: exact_match\n  Description: Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.\n  Required parameters:\n    - predictions: List of predicted texts.\n    - references: List of reference texts.\n  Optional parameters:\n    - regexes_to_ignore: Regex expressions of characters to ignore when calculating the exact matches.\n    - ignore_case: If true, turns everything to lowercase so that capitalization differences are ignored.\n    - ignore_numbers (bool): If true, removes all digits before comparing strings\n    - ignore_punctuation (bool): If true, removes punctuation before comparing strings.\n\n\n\n### User Query: \nWhat are the exact match scores for the predicted city names and their corresponding correct city names, given the following predictions and references: predictions = ['Tokyo', 'Paris', 'Rome'], references = ['Tokyo', 'Paris', 'Tokyo'], where the city names do not contain numbers and are in lowercase?",
    "answer": "Action: exact_match\nAction Input: {\n  \"predictions\": [\n    \"Tokyo\",\n    \"Paris\",\n    \"Rome\"\n  ],\n  \"references\": [\n    \"Tokyo\",\n    \"Paris\",\n    \"Tokyo\"\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: exact_match, recall\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: exact_match\n  Description: Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.\n  Required parameters:\n    - predictions: List of predicted texts.\n    - references: List of reference texts.\n  Optional parameters:\n    - regexes_to_ignore: Regex expressions of characters to ignore when calculating the exact matches.\n    - ignore_case: If true, turns everything to lowercase so that capitalization differences are ignored.\n    - ignore_numbers (bool): If true, removes all digits before comparing strings\n    - ignore_punctuation (bool): If true, removes punctuation before comparing strings.\n\nMetric name: recall\n  Description: Recall measures the proportion of actual positive instances that are correctly identified (Recall = TP / (TP + FN)), indicating the model's sensitivity.\n  Required parameters:\n    - predictions: Predicted labels.\n    - references: Ground Truth labels.\n  Optional parameters:\n    - average: This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data (e.g., 'binary', 'macro', 'micro', 'weighted', 'samples').\n    - labels: The set of labels to include when average is not set to 'binary', and the order of the labels if average is None.\n    - pos_label: The class to be considered the positive class, in the case where average is set to binary.\n    - sample_weight: Sample weights.\n\n\n\n### User Query: \nWhat are the exact match scores, recall scores with average='micro', labels=[0, 1, 2], pos_label=0, and sample_weight=[1, 1, 1] for the predicted labels and true labels in a multi-class classification task, given the following predictions and references: predictions = [0, 1, 2], references = [0, 0, 0]?",
    "answer": "Action: recall\nAction Input: {\n  \"predictions\": [\n    0,\n    1,\n    2\n  ],\n  \"references\": [\n    0,\n    0,\n    0\n  ],\n  \"average\": \"micro\",\n  \"labels\": [\n    0,\n    1,\n    2\n  ],\n  \"pos_label\": 0,\n  \"sample_weight\": [\n    1,\n    1,\n    1\n  ]\n}\n\nAction: recall\nAction Input: {\n  \"predictions\": [\n    0,\n    1,\n    2\n  ],\n  \"references\": [\n    0,\n    0,\n    0\n  ],\n  \"average\": \"micro\",\n  \"labels\": [\n    0,\n    1,\n    2\n  ],\n  \"pos_label\": 0,\n  \"sample_weight\": [\n    1,\n    1,\n    1\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: exact_match, recall, f1\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: exact_match\n  Description: Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.\n  Required parameters:\n    - predictions: List of predicted texts.\n    - references: List of reference texts.\n  Optional parameters:\n    - regexes_to_ignore: Regex expressions of characters to ignore when calculating the exact matches.\n    - ignore_case: If true, turns everything to lowercase so that capitalization differences are ignored.\n    - ignore_numbers (bool): If true, removes all digits before comparing strings\n    - ignore_punctuation (bool): If true, removes punctuation before comparing strings.\n\nMetric name: recall\n  Description: Recall measures the proportion of actual positive instances that are correctly identified (Recall = TP / (TP + FN)), indicating the model's sensitivity.\n  Required parameters:\n    - predictions: Predicted labels.\n    - references: Ground Truth labels.\n  Optional parameters:\n    - average: This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data (e.g., 'binary', 'macro', 'micro', 'weighted', 'samples').\n    - labels: The set of labels to include when average is not set to 'binary', and the order of the labels if average is None.\n    - pos_label: The class to be considered the positive class, in the case where average is set to binary.\n    - sample_weight: Sample weights.\n\nMetric name: f1\n  Description: The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.\n  Required parameters:\n    - predictions: The *N* predicted labels.\n    - references: The *N* true labels.\n  Optional parameters:\n    - average: The averaging method to use (e.g., 'macro', 'micro', 'weighted').\n\n\n\n### User Query: \nWhat are the exact match, recall, and F1 scores for the following predictions and references: predictions = ['Paris', 'London', 'Berlin', 'Rome'], references = ['Paris', 'London', 'Berlin', 'Rome'], average ='macro'?",
    "answer": "Action: exact_match\nAction Input: {\n  \"predictions\": [\n    \"Paris\",\n    \"London\",\n    \"Berlin\",\n    \"Rome\"\n  ],\n  \"references\": [\n    \"Paris\",\n    \"London\",\n    \"Berlin\",\n    \"Rome\"\n  ]\n}\n\nAction: recall\nAction Input: {\n  \"predictions\": [\n    0,\n    1,\n    1,\n    0\n  ],\n  \"references\": [\n    0,\n    1,\n    1,\n    0\n  ]\n}\n\nAction: f1\nAction Input: {\n  \"predictions\": [\n    0,\n    1,\n    1,\n    0\n  ],\n  \"references\": [\n    0,\n    1,\n    1,\n    0\n  ],\n  \"average\": \"macro\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: exact_match\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: exact_match\n  Description: Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.\n  Required parameters:\n    - predictions: List of predicted texts.\n    - references: List of reference texts.\n  Optional parameters:\n    - regexes_to_ignore: Regex expressions of characters to ignore when calculating the exact matches.\n    - ignore_case: If true, turns everything to lowercase so that capitalization differences are ignored.\n    - ignore_numbers (bool): If true, removes all digits before comparing strings\n    - ignore_punctuation (bool): If true, removes punctuation before comparing strings.\n\n\n\n### User Query: \nWhat is the exact match percentage between the predicted texts ['Tokyo', 'Rome', 'Paris'] and the reference texts ['Tokyo', 'Rome', 'Paris']?",
    "answer": "Action: exact_match\nAction Input: {\n  \"predictions\": [\n    \"Tokyo\",\n    \"Rome\",\n    \"Paris\"\n  ],\n  \"references\": [\n    \"Tokyo\",\n    \"Rome\",\n    \"Paris\"\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: exact_match, recall, f1\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: exact_match\n  Description: Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.\n  Required parameters:\n    - predictions: List of predicted texts.\n    - references: List of reference texts.\n  Optional parameters:\n    - regexes_to_ignore: Regex expressions of characters to ignore when calculating the exact matches.\n    - ignore_case: If true, turns everything to lowercase so that capitalization differences are ignored.\n    - ignore_numbers (bool): If true, removes all digits before comparing strings\n    - ignore_punctuation (bool): If true, removes punctuation before comparing strings.\n\nMetric name: recall\n  Description: Recall measures the proportion of actual positive instances that are correctly identified (Recall = TP / (TP + FN)), indicating the model's sensitivity.\n  Required parameters:\n    - predictions: Predicted labels.\n    - references: Ground Truth labels.\n  Optional parameters:\n    - average: This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data (e.g., 'binary', 'macro', 'micro', 'weighted', 'samples').\n    - labels: The set of labels to include when average is not set to 'binary', and the order of the labels if average is None.\n    - pos_label: The class to be considered the positive class, in the case where average is set to binary.\n    - sample_weight: Sample weights.\n\nMetric name: f1\n  Description: The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.\n  Required parameters:\n    - predictions: The *N* predicted labels.\n    - references: The *N* true labels.\n  Optional parameters:\n    - average: The averaging method to use (e.g., 'macro', 'micro', 'weighted').\n\n\n\n### User Query: \nWhat are the F1 scores for the predicted labels [0, 1, 1, 0, 0, 0] and true labels [0, 1, 1, 0, 0, 0] with average set to weighted, and also the exact match scores for the predicted city names ['Paris', 'New York', 'Chicago', 'Paris', 'New York', 'Chicago'] and references ['Paris', 'New York', 'Chicago', 'Paris', 'New York', 'Chicago'] with regexes_to_ignore set to ['\\d', ','] and ignore_case set to True, ignore_numbers set to True, ignore_punctuation set to True, and also the recall scores for the predicted labels [1, 0, 1, 1, 0, 1] and true labels [1, 0, 1, 1, 0, 1] with average set to macro, labels set to [0, 1, 1, 0, 1, 0], pos_label set to 1, and sample_weight set to [0.1, 0.2, 0.3, 0.1, 0.1, 0.2]?",
    "answer": "Action: f1\nAction Input: {\n  \"predictions\": [\n    0,\n    1,\n    1,\n    0,\n    0,\n    0\n  ],\n  \"references\": [\n    0,\n    1,\n    1,\n    0,\n    0,\n    0\n  ],\n  \"average\": \"weighted\"\n}\n\nAction: exact_match\nAction Input: {\n  \"predictions\": [\n    \"Paris\",\n    \"New York\",\n    \"Chicago\",\n    \"Paris\",\n    \"New York\",\n    \"Chicago\"\n  ],\n  \"references\": [\n    \"Paris\",\n    \"New York\",\n    \"Chicago\",\n    \"Paris\",\n    \"New York\",\n    \"Chicago\"\n  ],\n  \"regexes_to_ignore\": [\n    \"\\\\d\",\n    \",\"\n  ],\n  \"ignore_case\": \"true\",\n  \"ignore_numbers\": \"true\",\n  \"ignore_punctuation\": \"true\"\n}\n\nAction: recall\nAction Input: {\n  \"predictions\": [\n    1,\n    0,\n    1,\n    1,\n    0,\n    1\n  ],\n  \"references\": [\n    1,\n    0,\n    1,\n    1,\n    0,\n    1\n  ],\n  \"average\": \"macro\",\n  \"labels\": [\n    0,\n    1,\n    1,\n    0,\n    1,\n    0\n  ],\n  \"pos_label\": 1,\n  \"sample_weight\": [\n    0.1,\n    0.2,\n    0.3,\n    0.1,\n    0.1,\n    0.2\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: exact_match, recall, f1\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: exact_match\n  Description: Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.\n  Required parameters:\n    - predictions: List of predicted texts.\n    - references: List of reference texts.\n  Optional parameters:\n    - regexes_to_ignore: Regex expressions of characters to ignore when calculating the exact matches.\n    - ignore_case: If true, turns everything to lowercase so that capitalization differences are ignored.\n    - ignore_numbers (bool): If true, removes all digits before comparing strings\n    - ignore_punctuation (bool): If true, removes punctuation before comparing strings.\n\nMetric name: recall\n  Description: Recall measures the proportion of actual positive instances that are correctly identified (Recall = TP / (TP + FN)), indicating the model's sensitivity.\n  Required parameters:\n    - predictions: Predicted labels.\n    - references: Ground Truth labels.\n  Optional parameters:\n    - average: This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data (e.g., 'binary', 'macro', 'micro', 'weighted', 'samples').\n    - labels: The set of labels to include when average is not set to 'binary', and the order of the labels if average is None.\n    - pos_label: The class to be considered the positive class, in the case where average is set to binary.\n    - sample_weight: Sample weights.\n\nMetric name: f1\n  Description: The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.\n  Required parameters:\n    - predictions: The *N* predicted labels.\n    - references: The *N* true labels.\n  Optional parameters:\n    - average: The averaging method to use (e.g., 'macro', 'micro', 'weighted').\n\n\n\n### User Query: \nWhat is the exact match score between the predictions ['Paris', 'Berlin', 'Rome'] and the references ['Paris', 'Berlin', 'Rome'], ignoring numbers, punctuation, case sensitivity, and the regex expressions '\\d' and '[^a-zA-Z ]', and what is the recall score between the predictions [1, 0, 1] and the references [1, 1, 1], with average set to'macro', and what is the F1 score between the predictions [1, 0, 1] and the references [1, 1, 0] with average set to'micro'?",
    "answer": "Action: exact_match\nAction Input: {\n  \"predictions\": [\n    \"Paris\",\n    \"Berlin\",\n    \"Rome\"\n  ],\n  \"references\": [\n    \"Paris\",\n    \"Berlin\",\n    \"Rome\"\n  ],\n  \"regexes_to_ignore\": [\n    \"\\\\d\",\n    \"[^a-zA-Z ]\"\n  ],\n  \"ignore_case\": true,\n  \"ignore_numbers\": true,\n  \"ignore_punctuation\": true\n}\n\nAction: recall\nAction Input: {\n  \"predictions\": [\n    1,\n    0,\n    1\n  ],\n  \"references\": [\n    1,\n    1,\n    1\n  ],\n  \"average\": \"macro\",\n  \"labels\": [\n    0,\n    1\n  ],\n  \"pos_label\": 1,\n  \"sample_weight\": [\n    1,\n    1,\n    1\n  ]\n}\n\nAction: f1\nAction Input: {\n  \"predictions\": [\n    1,\n    0,\n    1\n  ],\n  \"references\": [\n    1,\n    1,\n    0\n  ],\n  \"average\": \"micro\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: exact_match, recall\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: exact_match\n  Description: Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.\n  Required parameters:\n    - predictions: List of predicted texts.\n    - references: List of reference texts.\n  Optional parameters:\n    - regexes_to_ignore: Regex expressions of characters to ignore when calculating the exact matches.\n    - ignore_case: If true, turns everything to lowercase so that capitalization differences are ignored.\n    - ignore_numbers (bool): If true, removes all digits before comparing strings\n    - ignore_punctuation (bool): If true, removes punctuation before comparing strings.\n\nMetric name: recall\n  Description: Recall measures the proportion of actual positive instances that are correctly identified (Recall = TP / (TP + FN)), indicating the model's sensitivity.\n  Required parameters:\n    - predictions: Predicted labels.\n    - references: Ground Truth labels.\n  Optional parameters:\n    - average: This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data (e.g., 'binary', 'macro', 'micro', 'weighted', 'samples').\n    - labels: The set of labels to include when average is not set to 'binary', and the order of the labels if average is None.\n    - pos_label: The class to be considered the positive class, in the case where average is set to binary.\n    - sample_weight: Sample weights.\n\n\n\n### User Query: \nWhat are the exact match scores for the predicted city names ['Paris', 'Rome', 'Berlin', 'Amsterdam', 'Barcelona', 'Dublin'] and references ['Paris', 'Rome', 'Berlin', 'Amsterdam', 'Madrid', 'Dublin'] with regexes_to_ignore set to ['\\d', ','] and ignore_case set to True, ignore_numbers set to True, ignore_punctuation set to True, and also the recall scores for the predicted labels [1, 1, 0, 1, 0, 0] and true labels [1, 1, 1, 1, 0, 0] with average set to micro, labels set to [0, 1], pos_label set to 1, and sample_weight set to [0.2, 0.2, 0.2, 0.2, 0.2, 0.2]?",
    "answer": "Action: exact_match\nAction Input: {\n  \"predictions\": [\n    \"Paris\",\n    \"Rome\",\n    \"Berlin\",\n    \"Amsterdam\",\n    \"Barcelona\",\n    \"Dublin\"\n  ],\n  \"references\": [\n    \"Paris\",\n    \"Rome\",\n    \"Berlin\",\n    \"Amsterdam\",\n    \"Madrid\",\n    \"Dublin\"\n  ],\n  \"regexes_to_ignore\": [\n    \"\\\\d\",\n    \",\"\n  ],\n  \"ignore_case\": true,\n  \"ignore_numbers\": true,\n  \"ignore_punctuation\": true\n}\n\nAction: recall\nAction Input: {\n  \"predictions\": [\n    1,\n    1,\n    0,\n    1,\n    0,\n    0\n  ],\n  \"references\": [\n    1,\n    1,\n    1,\n    1,\n    0,\n    0\n  ],\n  \"average\": \"micro\",\n  \"labels\": [\n    0,\n    1\n  ],\n  \"pos_label\": 1,\n  \"sample_weight\": [\n    0.2,\n    0.2,\n    0.2,\n    0.2,\n    0.2,\n    0.2\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: recall\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: recall\n  Description: Recall measures the proportion of actual positive instances that are correctly identified (Recall = TP / (TP + FN)), indicating the model's sensitivity.\n  Required parameters:\n    - predictions: Predicted labels.\n    - references: Ground Truth labels.\n  Optional parameters:\n    - average: This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data (e.g., 'binary', 'macro', 'micro', 'weighted', 'samples').\n    - labels: The set of labels to include when average is not set to 'binary', and the order of the labels if average is None.\n    - pos_label: The class to be considered the positive class, in the case where average is set to binary.\n    - sample_weight: Sample weights.\n\n\n\n### User Query: \nWhat is the recall when the predictions are [0, 1, 1, 0, 1] and the references are [0, 1, 0, 0, 1]?",
    "answer": "Action: recall\nAction Input: {\n  \"predictions\": [\n    0,\n    1,\n    1,\n    0,\n    1\n  ],\n  \"references\": [\n    0,\n    1,\n    0,\n    0,\n    1\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: exact_match, recall, f1\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: exact_match\n  Description: Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.\n  Required parameters:\n    - predictions: List of predicted texts.\n    - references: List of reference texts.\n  Optional parameters:\n    - regexes_to_ignore: Regex expressions of characters to ignore when calculating the exact matches.\n    - ignore_case: If true, turns everything to lowercase so that capitalization differences are ignored.\n    - ignore_numbers (bool): If true, removes all digits before comparing strings\n    - ignore_punctuation (bool): If true, removes punctuation before comparing strings.\n\nMetric name: recall\n  Description: Recall measures the proportion of actual positive instances that are correctly identified (Recall = TP / (TP + FN)), indicating the model's sensitivity.\n  Required parameters:\n    - predictions: Predicted labels.\n    - references: Ground Truth labels.\n  Optional parameters:\n    - average: This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data (e.g., 'binary', 'macro', 'micro', 'weighted', 'samples').\n    - labels: The set of labels to include when average is not set to 'binary', and the order of the labels if average is None.\n    - pos_label: The class to be considered the positive class, in the case where average is set to binary.\n    - sample_weight: Sample weights.\n\nMetric name: f1\n  Description: The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.\n  Required parameters:\n    - predictions: The *N* predicted labels.\n    - references: The *N* true labels.\n  Optional parameters:\n    - average: The averaging method to use (e.g., 'macro', 'micro', 'weighted').\n\n\n\n### User Query: \nWhat are the exact match scores for the predicted texts [\"New York\", \"New York\", \"Paris\"] and [\"Paris\", \"London\", \"Paris\"], the recall scores for the predicted labels [1, 0, 0] and [0, 1, 0] and the true labels [1, 0, 0] and [0, 1, 1], and the F1 scores for the predicted labels [0, 1, 1] and [1, 1, 0] and the true labels [0, 1, 1] and [1, 1, 0] when ignoring case, numbers, and punctuation, and ignoring the characters 'a' and 'e', and with sample weights [1.0, 1.0, 1.0] and [1.0, 1.0, 1.0] respectively, and what is the effect of ignoring the characters 'o' and 'i' on the exact match scores?",
    "answer": "Action: exact_match\nAction Input: {\n  \"predictions\": [\n    \"New York\",\n    \"New York\",\n    \"Paris\"\n  ],\n  \"references\": [\n    \"Paris\",\n    \"London\",\n    \"Paris\"\n  ],\n  \"regexes_to_ignore\": \"['a', 'e']\",\n  \"ignore_case\": \"true\",\n  \"ignore_numbers\": \"true\",\n  \"ignore_punctuation\": \"true\"\n}\n\nAction: exact_match\nAction Input: {\n  \"predictions\": [\n    \"New York\",\n    \"New York\",\n    \"Paris\"\n  ],\n  \"references\": [\n    \"Paris\",\n    \"London\",\n    \"Paris\"\n  ],\n  \"regexes_to_ignore\": \"['o', 'i']\",\n  \"ignore_case\": \"true\",\n  \"ignore_numbers\": \"true\",\n  \"ignore_punctuation\": \"true\"\n}\n\nAction: recall\nAction Input: {\n  \"predictions\": [\n    1,\n    0,\n    0\n  ],\n  \"references\": [\n    1,\n    0,\n    0\n  ]\n}\n\nAction: recall\nAction Input: {\n  \"predictions\": [\n    0,\n    1,\n    0\n  ],\n  \"references\": [\n    0,\n    1,\n    1\n  ],\n  \"average\": \"macro\",\n  \"labels\": [\n    0,\n    1\n  ],\n  \"pos_label\": 1,\n  \"sample_weight\": [\n    1.0,\n    1.0,\n    1.0\n  ]\n}\n\nAction: f1\nAction Input: {\n  \"predictions\": [\n    0,\n    1,\n    1\n  ],\n  \"references\": [\n    0,\n    1,\n    1\n  ]\n}\n\nAction: f1\nAction Input: {\n  \"predictions\": [\n    1,\n    1,\n    0\n  ],\n  \"references\": [\n    1,\n    1,\n    0\n  ],\n  \"average\": \"micro\"\n}\n\nAction: exact_match\nAction Input: {\n  \"predictions\": [\n    \"New York\",\n    \"New York\",\n    \"Paris\"\n  ],\n  \"references\": [\n    \"Paris\",\n    \"London\",\n    \"Paris\"\n  ],\n  \"regexes_to_ignore\": \"['o', 'i']\",\n  \"ignore_case\": \"true\",\n  \"ignore_numbers\": \"true\",\n  \"ignore_punctuation\": \"true\"\n}\n\nAction: exact_match\nAction Input: {\n  \"predictions\": [\n    \"New York\",\n    \"New York\",\n    \"Paris\"\n  ],\n  \"references\": [\n    \"Paris\",\n    \"London\",\n    \"Paris\"\n  ],\n  \"regexes_to_ignore\": \"['o', 'i']\",\n  \"ignore_case\": \"true\",\n  \"ignore_numbers\": \"true\",\n  \"ignore_punctuation\": \"true\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: recall\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: recall\n  Description: Recall measures the proportion of actual positive instances that are correctly identified (Recall = TP / (TP + FN)), indicating the model's sensitivity.\n  Required parameters:\n    - predictions: Predicted labels.\n    - references: Ground Truth labels.\n  Optional parameters:\n    - average: This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data (e.g., 'binary', 'macro', 'micro', 'weighted', 'samples').\n    - labels: The set of labels to include when average is not set to 'binary', and the order of the labels if average is None.\n    - pos_label: The class to be considered the positive class, in the case where average is set to binary.\n    - sample_weight: Sample weights.\n\n\n\n### User Query: \nWhat are the recall scores for the following predicted and actual labels: predictions = [1, 1, 0, 1, 0], references = [1, 0, 1, 1, 0], average='micro', labels=[0, 1], pos_label=1, sample_weight=[0.7, 0.3, 0.9, 0.1, 0.5]?",
    "answer": "Action: recall\nAction Input: {\n  \"predictions\": [\n    1,\n    1,\n    0,\n    1,\n    0\n  ],\n  \"references\": [\n    1,\n    0,\n    1,\n    1,\n    0\n  ],\n  \"average\": \"micro\",\n  \"labels\": [\n    0,\n    1\n  ],\n  \"pos_label\": 1,\n  \"sample_weight\": [\n    0.7,\n    0.3,\n    0.9,\n    0.1,\n    0.5\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: perplexity\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: perplexity\n  Description: Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.\n  Required parameters:\n    - predictions: input text, where each separate text snippet is one list entry.\n    - model_id: model_id (str): model used for calculating Perplexity. NOTE: Perplexity can only be calculated for causal language models, such as gpt2, causal variations of bert, causal versions of t5. (e.g., 'gpt2').\n  Optional parameters:\n    - batch_size: the batch size to run texts through the model.\n    - add_start_token: whether to add the start token to the texts, so the perplexity can include the probability of the first word.\n\n\n\n### User Query: \nWhat is the perplexity of the following texts: \"The sun sets over the ocean.\", \"The stars shine brightly in the night sky.\", \"The world is full of wonder.\" and \"The mountains touch the clouds.\" using model gpt2 with a batch size of 4 and add the start token?",
    "answer": "Action: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The sun sets over the ocean.\",\n    \"The stars shine brightly in the night sky.\",\n    \"The world is full of wonder.\",\n    \"The mountains touch the clouds.\"\n  ],\n  \"model_id\": \"gpt2\",\n  \"batch_size\": 4,\n  \"add_start_token\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: perplexity\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: perplexity\n  Description: Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.\n  Required parameters:\n    - predictions: input text, where each separate text snippet is one list entry.\n    - model_id: model_id (str): model used for calculating Perplexity. NOTE: Perplexity can only be calculated for causal language models, such as gpt2, causal variations of bert, causal versions of t5. (e.g., 'gpt2').\n  Optional parameters:\n    - batch_size: the batch size to run texts through the model.\n    - add_start_token: whether to add the start token to the texts, so the perplexity can include the probability of the first word.\n\n\n\n### User Query: \nHow does the perplexity of gpt2 model change when generating these texts: \"The sun is shining brightly in the clear blue sky.\" and \"The baby is laughing at the clown's silly antics.\" compared to these texts: \"The dog is running quickly through the park.\", \"The kids are playing happily together.\" using the model with default batch size and start tokens added?",
    "answer": "Action: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The sun is shining brightly in the clear blue sky.\",\n    \"The baby is laughing at the clown's silly antics.\"\n  ],\n  \"model_id\": \"gpt2\"\n}\n\nAction: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The dog is running quickly through the park.\",\n    \"The kids are playing happily together.\"\n  ],\n  \"model_id\": \"gpt2\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: perplexity\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: perplexity\n  Description: Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.\n  Required parameters:\n    - predictions: input text, where each separate text snippet is one list entry.\n    - model_id: model_id (str): model used for calculating Perplexity. NOTE: Perplexity can only be calculated for causal language models, such as gpt2, causal variations of bert, causal versions of t5. (e.g., 'gpt2').\n  Optional parameters:\n    - batch_size: the batch size to run texts through the model.\n    - add_start_token: whether to add the start token to the texts, so the perplexity can include the probability of the first word.\n\n\n\n### User Query: \nWhat is the perplexity of the texts: \"A quiet moment of solitude is a rare treasure.\", \"The sound of a gentle stream is calming.\", \"The world is full of subtle beauty.\", \"The smell of freshly baked cookies wafts through the air.\", \"The taste of a cold glass of lemonade is refreshing.\", \"The feeling of walking in a lush forest is invigorating.\" using model gpt2 with a batch size of 16 and add the start token.",
    "answer": "Action: perplexity\nAction Input: {\n  \"predictions\": [\n    \"A quiet moment of solitude is a rare treasure.\",\n    \"The sound of a gentle stream is calming.\",\n    \"The world is full of subtle beauty.\",\n    \"The smell of freshly baked cookies wafts through the air.\",\n    \"The taste of a cold glass of lemonade is refreshing.\",\n    \"The feeling of walking in a lush forest is invigorating.\"\n  ],\n  \"model_id\": \"gpt2\",\n  \"batch_size\": 16,\n  \"add_start_token\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: perplexity\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: perplexity\n  Description: Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.\n  Required parameters:\n    - predictions: input text, where each separate text snippet is one list entry.\n    - model_id: model_id (str): model used for calculating Perplexity. NOTE: Perplexity can only be calculated for causal language models, such as gpt2, causal variations of bert, causal versions of t5. (e.g., 'gpt2').\n  Optional parameters:\n    - batch_size: the batch size to run texts through the model.\n    - add_start_token: whether to add the start token to the texts, so the perplexity can include the probability of the first word.\n\n\n\n### User Query: \nWhat is the perplexity of the model with model_id 'gpt2' on the input texts ['The fog slowly rolls in off the ocean', 'A lone seagull cries out in the distance', 'The salty spray misting my skin'] with batch_size 32 and add_start_token true?",
    "answer": "Action: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The fog slowly rolls in off the ocean\",\n    \"A lone seagull cries out in the distance\",\n    \"The salty spray misting my skin\"\n  ],\n  \"model_id\": \"gpt2\",\n  \"batch_size\": 32,\n  \"add_start_token\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: perplexity\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: perplexity\n  Description: Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.\n  Required parameters:\n    - predictions: input text, where each separate text snippet is one list entry.\n    - model_id: model_id (str): model used for calculating Perplexity. NOTE: Perplexity can only be calculated for causal language models, such as gpt2, causal variations of bert, causal versions of t5. (e.g., 'gpt2').\n  Optional parameters:\n    - batch_size: the batch size to run texts through the model.\n    - add_start_token: whether to add the start token to the texts, so the perplexity can include the probability of the first word.\n\n\n\n### User Query: \nWhat is the perplexity of the texts: \"The morning sun rises over the mountains.\", \"A gentle breeze rustles the leaves.\", \"The world is full of hidden patterns.\", \"The sound of a piano echoes through the hall.\", \"The taste of a ripe peach is sweet.\", \"The feeling of sand between my toes is calming.\" using model gpt2 with a batch size of 10 and add the start token.",
    "answer": "Action: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The morning sun rises over the mountains.\",\n    \"A gentle breeze rustles the leaves.\",\n    \"The world is full of hidden patterns.\",\n    \"The sound of a piano echoes through the hall.\",\n    \"The taste of a ripe peach is sweet.\",\n    \"The feeling of sand between my toes is calming.\"\n  ],\n  \"model_id\": \"gpt2\",\n  \"batch_size\": 10,\n  \"add_start_token\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: perplexity\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: perplexity\n  Description: Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.\n  Required parameters:\n    - predictions: input text, where each separate text snippet is one list entry.\n    - model_id: model_id (str): model used for calculating Perplexity. NOTE: Perplexity can only be calculated for causal language models, such as gpt2, causal variations of bert, causal versions of t5. (e.g., 'gpt2').\n  Optional parameters:\n    - batch_size: the batch size to run texts through the model.\n    - add_start_token: whether to add the start token to the texts, so the perplexity can include the probability of the first word.\n\n\n\n### User Query: \nHow does the perplexity of the model 'gpt2' compare when generating texts with a batch size of 64 and add_start_token set to false versus when generating texts with a batch size of 64 and add_start_token set to true for the texts \"The morning sunshine brings new life.\", \"The warmth of the sun on my skin feels amazing.\", \"The smell of freshly baked cookies is incredible.\", \"The taste of a juicy apple is delicious.\", \"The sound of birds chirping is calming.\", \"The beauty of a sunrise is breathtaking.\"?",
    "answer": "Action: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The morning sunshine brings new life.\",\n    \"The warmth of the sun on my skin feels amazing.\",\n    \"The smell of freshly baked cookies is incredible.\",\n    \"The taste of a juicy apple is delicious.\",\n    \"The sound of birds chirping is calming.\",\n    \"The beauty of a sunrise is breathtaking.\"\n  ],\n  \"model_id\": \"gpt2\",\n  \"batch_size\": 64,\n  \"add_start_token\": false\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: perplexity\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: perplexity\n  Description: Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.\n  Required parameters:\n    - predictions: input text, where each separate text snippet is one list entry.\n    - model_id: model_id (str): model used for calculating Perplexity. NOTE: Perplexity can only be calculated for causal language models, such as gpt2, causal variations of bert, causal versions of t5. (e.g., 'gpt2').\n  Optional parameters:\n    - batch_size: the batch size to run texts through the model.\n    - add_start_token: whether to add the start token to the texts, so the perplexity can include the probability of the first word.\n\n\n\n### User Query: \nWhat is the perplexity of the following texts: \"The morning light creeps over the hills.\", \"A gentle brook babbles by the path.\", \"The world is full of hidden treasures.\", \"The mountains stand tall and proud.\", \"The river flows gently to the sea.\" using model gpt2 with a batch size of 8 and add the start token.",
    "answer": "Action: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The morning light creeps over the hills.\",\n    \"A gentle brook babbles by the path.\",\n    \"The world is full of hidden treasures.\",\n    \"The mountains stand tall and proud.\",\n    \"The river flows gently to the sea.\"\n  ],\n  \"model_id\": \"gpt2\",\n  \"batch_size\": 8,\n  \"add_start_token\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: perplexity\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: perplexity\n  Description: Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.\n  Required parameters:\n    - predictions: input text, where each separate text snippet is one list entry.\n    - model_id: model_id (str): model used for calculating Perplexity. NOTE: Perplexity can only be calculated for causal language models, such as gpt2, causal variations of bert, causal versions of t5. (e.g., 'gpt2').\n  Optional parameters:\n    - batch_size: the batch size to run texts through the model.\n    - add_start_token: whether to add the start token to the texts, so the perplexity can include the probability of the first word.\n\n\n\n### User Query: \nWhat is the perplexity of the model with model_id 'gpt2' on the input texts ['The morning dew glistens on the green grass', 'A warm cup of coffee on a chilly winter morning', 'The soft chirping of birds fills the air'] with batch_size 32 and add_start_token true?",
    "answer": "Action: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The morning dew glistens on the green grass\",\n    \"A warm cup of coffee on a chilly winter morning\",\n    \"The soft chirping of birds fills the air\"\n  ],\n  \"model_id\": \"gpt2\",\n  \"batch_size\": 32,\n  \"add_start_token\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: perplexity\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: perplexity\n  Description: Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.\n  Required parameters:\n    - predictions: input text, where each separate text snippet is one list entry.\n    - model_id: model_id (str): model used for calculating Perplexity. NOTE: Perplexity can only be calculated for causal language models, such as gpt2, causal variations of bert, causal versions of t5. (e.g., 'gpt2').\n  Optional parameters:\n    - batch_size: the batch size to run texts through the model.\n    - add_start_token: whether to add the start token to the texts, so the perplexity can include the probability of the first word.\n\n\n\n### User Query: \nWhat is the perplexity of these texts: \"The flowers bloom in the spring.\", \"The baby takes its first steps.\", \"The sun shines brightly in the morning.\", \"The kids play outside all day.\", \"The world is a beautiful place.\" and \"The stars twinkle in the night sky\" using the gpt2 model with a batch size of 32 and start tokens added, and then for these texts: \"The dog runs quickly through the park.\", \"The cat purrs contentedly on my lap.\", \"The baby laughs at the clown's antics.\", \"The flowers smell so sweet.\", \"The ocean waves crash against the shore.\" and \"The snowflakes fall gently from the sky\" using the gpt2 model with a batch size of 32 and start tokens added?",
    "answer": "Action: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The flowers bloom in the spring.\",\n    \"The baby takes its first steps.\",\n    \"The sun shines brightly in the morning.\",\n    \"The kids play outside all day.\",\n    \"The world is a beautiful place.\",\n    \"The stars twinkle in the night sky\"\n  ],\n  \"model_id\": \"gpt2\",\n  \"batch_size\": 32,\n  \"add_start_token\": true\n}\n\nAction: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The dog runs quickly through the park.\",\n    \"The cat purrs contentedly on my lap.\",\n    \"The baby laughs at the clown's antics.\",\n    \"The flowers smell so sweet.\",\n    \"The ocean waves crash against the shore.\",\n    \"The snowflakes fall gently from the sky\"\n  ],\n  \"model_id\": \"gpt2\",\n  \"batch_size\": 32,\n  \"add_start_token\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: perplexity\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: perplexity\n  Description: Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.\n  Required parameters:\n    - predictions: input text, where each separate text snippet is one list entry.\n    - model_id: model_id (str): model used for calculating Perplexity. NOTE: Perplexity can only be calculated for causal language models, such as gpt2, causal variations of bert, causal versions of t5. (e.g., 'gpt2').\n  Optional parameters:\n    - batch_size: the batch size to run texts through the model.\n    - add_start_token: whether to add the start token to the texts, so the perplexity can include the probability of the first word.\n\n\n\n### User Query: \nWhat is the perplexity of the model 'gpt2' for the texts \"The sky is a brilliant blue.\", \"The trees sway gently in the breeze.\", \"The flowers bloom in the garden.\", \"The birds sing sweet melodies.\", \"The sun shines brightly overhead.\", \"The world is full of beauty.\" and \"The morning light creeps over the horizon.\", \"The stars twinkle in the night sky.\", \"The moon glows with a soft light.\", \"The forest is a peaceful place.\", \"The river flows smoothly to the sea.\" when using a batch size of 32 and add_start_token set to true?",
    "answer": "Action: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The sky is a brilliant blue.\",\n    \"The trees sway gently in the breeze.\",\n    \"The flowers bloom in the garden.\",\n    \"The birds sing sweet melodies.\",\n    \"The sun shines brightly overhead.\",\n    \"The world is full of beauty.\",\n    \"The morning light creeps over the horizon.\",\n    \"The stars twinkle in the night sky.\",\n    \"The moon glows with a soft light.\",\n    \"The forest is a peaceful place.\",\n    \"The river flows smoothly to the sea.\"\n  ],\n  \"model_id\": \"gpt2\",\n  \"batch_size\": 32,\n  \"add_start_token\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: exact_match, recall, f1\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: exact_match\n  Description: Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.\n  Required parameters:\n    - predictions: List of predicted texts.\n    - references: List of reference texts.\n  Optional parameters:\n    - regexes_to_ignore: Regex expressions of characters to ignore when calculating the exact matches.\n    - ignore_case: If true, turns everything to lowercase so that capitalization differences are ignored.\n    - ignore_numbers (bool): If true, removes all digits before comparing strings\n    - ignore_punctuation (bool): If true, removes punctuation before comparing strings.\n\nMetric name: recall\n  Description: Recall measures the proportion of actual positive instances that are correctly identified (Recall = TP / (TP + FN)), indicating the model's sensitivity.\n  Required parameters:\n    - predictions: Predicted labels.\n    - references: Ground Truth labels.\n  Optional parameters:\n    - average: This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data (e.g., 'binary', 'macro', 'micro', 'weighted', 'samples').\n    - labels: The set of labels to include when average is not set to 'binary', and the order of the labels if average is None.\n    - pos_label: The class to be considered the positive class, in the case where average is set to binary.\n    - sample_weight: Sample weights.\n\nMetric name: f1\n  Description: The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.\n  Required parameters:\n    - predictions: The *N* predicted labels.\n    - references: The *N* true labels.\n  Optional parameters:\n    - average: The averaging method to use (e.g., 'macro', 'micro', 'weighted').\n\n\n\n### User Query: \nWhat are the exact match scores between the predictions ['Paris', 'Lyon', 'Marseille'] and the references ['Paris', 'Lyon', 'Marseille'], the recall score between the predictions [0, 0, 1] and the references [0, 1, 1] with average'macro', labels [0, 1], pos_label 1, sample_weight [1, 1, 2], and the F1 score between the predictions ['apple', 'banana', 'cherry'] and the references ['apple', 'banana', 'cherry']?",
    "answer": "Action: exact_match\nAction Input: {\n  \"predictions\": [\n    \"Paris\",\n    \"Lyon\",\n    \"Marseille\"\n  ],\n  \"references\": [\n    \"Paris\",\n    \"Lyon\",\n    \"Marseille\"\n  ]\n}\n\nAction: recall\nAction Input: {\n  \"predictions\": [\n    0,\n    0,\n    1\n  ],\n  \"references\": [\n    0,\n    1,\n    1\n  ],\n  \"average\": \"macro\",\n  \"labels\": [\n    0,\n    1\n  ],\n  \"pos_label\": 1,\n  \"sample_weight\": [\n    1,\n    1,\n    2\n  ]\n}\n\nAction: f1\nAction Input: {\n  \"predictions\": [\n    0,\n    0,\n    1\n  ],\n  \"references\": [\n    0,\n    1,\n    1\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bertscore, recall, f1, bleu\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bertscore\n  Description: BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.\n  Required parameters:\n    - predictions: The generated sentences.\n    - references: The reference sentences.\n    - lang: The language of the input text (e.g., 'en' for English).\n  Optional parameters:\n    - idf: Whether to use inverse document frequency reweighting (default: false).\n\nMetric name: recall\n  Description: Recall measures the proportion of actual positive instances that are correctly identified (Recall = TP / (TP + FN)), indicating the model's sensitivity.\n  Required parameters:\n    - predictions: Predicted labels.\n    - references: Ground Truth labels.\n  Optional parameters:\n    - average: This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data (e.g., 'binary', 'macro', 'micro', 'weighted', 'samples').\n    - labels: The set of labels to include when average is not set to 'binary', and the order of the labels if average is None.\n    - pos_label: The class to be considered the positive class, in the case where average is set to binary.\n    - sample_weight: Sample weights.\n\nMetric name: f1\n  Description: The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.\n  Required parameters:\n    - predictions: The *N* predicted labels.\n    - references: The *N* true labels.\n  Optional parameters:\n    - average: The averaging method to use (e.g., 'macro', 'micro', 'weighted').\n\nMetric name: bleu\n  Description: BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of *N* machine translation by comparing a candidate translation against *N* reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.\n  Required parameters:\n    - predictions: List of *N* translations to score\n    - references: list of *N* reference translations or a list of *N* lists of reference translations.\n  Optional parameters:\n    - max_order: Maximum n-gram order to consider (default: 4).\n    - smooth: Whether to apply smoothing (default: false).\n\n\n\n### User Query: \nWhat are the BLEU scores with max_order 4 and smoothing of [\"The dog is running quickly\", \"The cat is playing with a ball\"] against [\"The dog is running slowly\", \"The cat is playing with a toy\"], [\"The dog is running quickly\", \"The cat is playing with a toy\"], and [\"The dog is running slowly\", \"The cat is playing with a toy\"], and the BERTScore precision, recall, and F1 scores with idf reweighting of [\"The dog is running quickly\", \"The cat is playing with a ball\"] against [\"The dog is running slowly\", \"The cat is playing with a toy\"], [\"The dog is running quickly\", \"The cat is playing with a toy\"], [\"The dog is running slowly\", \"The cat is playing with a toy\"], and the BERTScore of [\"The dog is running quickly\", \"The cat is playing with a ball\"] against [\"The dog is running slowly\", \"The cat is playing with a toy\"], [\"The dog is running quickly\", \"The cat is playing with a toy\"], with idf reweighting?",
    "answer": "Action: bertscore\nAction Input: {\n  \"predictions\": [\n    \"The dog is running quickly\",\n    \"The cat is playing with a ball\"\n  ],\n  \"references\": [\n    [\n      \"The dog is running slowly\",\n      \"The cat is playing with a toy\"\n    ],\n    [\n      \"The dog is running quickly\",\n      \"The cat is playing with a toy\"\n    ]\n  ],\n  \"lang\": \"en\",\n  \"idf\": true\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"The dog is running quickly\",\n    \"The cat is playing with a ball\"\n  ],\n  \"references\": [\n    [\n      \"The dog is running quickly\",\n      \"The cat is playing with a toy\"\n    ],\n    [\n      \"The dog is running slowly\",\n      \"The cat is playing with a toy\"\n    ]\n  ],\n  \"lang\": \"en\",\n  \"idf\": true\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"The dog is running quickly\",\n    \"The cat is playing with a ball\"\n  ],\n  \"references\": [\n    [\n      \"The dog is running slowly\",\n      \"The cat is playing with a toy\"\n    ],\n    [\n      \"The dog is running slowly\",\n      \"The cat is playing with a toy\"\n    ]\n  ],\n  \"lang\": \"en\",\n  \"idf\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: f1\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: f1\n  Description: The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.\n  Required parameters:\n    - predictions: The *N* predicted labels.\n    - references: The *N* true labels.\n  Optional parameters:\n    - average: The averaging method to use (e.g., 'macro', 'micro', 'weighted').\n\n\n\n### User Query: \nWhat are the F1 scores of a model that predicted the following product categories:'smartphone', 'gaming console', 'laptop', 'headphones','smartwatch', and 'tablet', and the true categories are'smartphone', 'console', 'laptop', 'headphones', 'watch', 'gadget' respectively?",
    "answer": "Action: f1\nAction Input: {\n  \"predictions\": [\n    1,\n    2,\n    3,\n    4,\n    5,\n    6\n  ],\n  \"references\": [\n    1,\n    2,\n    3,\n    4,\n    5,\n    7\n  ],\n  \"average\": \"macro\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: accuracy, f1\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: accuracy\n  Description: Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.\n  Required parameters:\n    - predictions: Predicted labels\n    - references: Ground truth labels.\n  Optional parameters:\n    - normalize: If set to false, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples\n    - sample_weight: Predicted labels\n\nMetric name: f1\n  Description: The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.\n  Required parameters:\n    - predictions: The *N* predicted labels.\n    - references: The *N* true labels.\n  Optional parameters:\n    - average: The averaging method to use (e.g., 'macro', 'micro', 'weighted').\n\n\n\n### User Query: \nWhat are the accuracy and F1 score of a model with predictions [1, 0, 1, 0] and references [1, 1, 0, 0] when normalize is False, sample_weight is [0.4, 0.3, 0.1, 0.2], and average is'micro'?",
    "answer": "Action: accuracy\nAction Input: {\n  \"predictions\": [\n    1,\n    0,\n    1,\n    0\n  ],\n  \"references\": [\n    1,\n    1,\n    0,\n    0\n  ],\n  \"normalize\": false,\n  \"sample_weight\": [\n    0.4,\n    0.3,\n    0.1,\n    0.2\n  ]\n}\n\nAction: f1\nAction Input: {\n  \"predictions\": [\n    1,\n    0,\n    1,\n    0\n  ],\n  \"references\": [\n    1,\n    1,\n    0,\n    0\n  ],\n  \"average\": \"micro\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bertscore, bleu, rouge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bertscore\n  Description: BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.\n  Required parameters:\n    - predictions: The generated sentences.\n    - references: The reference sentences.\n    - lang: The language of the input text (e.g., 'en' for English).\n  Optional parameters:\n    - idf: Whether to use inverse document frequency reweighting (default: false).\n\nMetric name: bleu\n  Description: BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of *N* machine translation by comparing a candidate translation against *N* reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.\n  Required parameters:\n    - predictions: List of *N* translations to score\n    - references: list of *N* reference translations or a list of *N* lists of reference translations.\n  Optional parameters:\n    - max_order: Maximum n-gram order to consider (default: 4).\n    - smooth: Whether to apply smoothing (default: false).\n\nMetric name: rouge\n  Description: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.\n  Required parameters:\n    - predictions: list of *N* predictions to score. Each prediction should be a string with tokens separated by spaces\n    - references: list of *N* references (one for prediction) or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.\n  Optional parameters:\n    - rouge_types: Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).\n    - use_aggregator: If true, returns aggregated scores; if false, returns individual scores for each prediction-reference pair. Defaults to true.\n    - use_stemmer: If true, applies a Porter stemmer to normalize words before comparison. Defaults to false.\n\n\n\n### User Query: \nWhat are the ROUGE scores for the following predictions and references: predictions=['the cat is very happy', 'the dog is very sleepy'], references=[['the cat is very happy today', 'the cat is very happy now'], ['the dog is very sleepy now', 'the dog is very sleepy yesterday']], rouge_types=['rouge1', 'rouge2'], use_aggregator=True, use_stemmer=False, and the BLEU scores for these predictions: predictions=['the house is blue', 'the apple is red'], references=[['the house is blue and big', 'the house is old'], ['the apple is red and juicy', 'the apple is sweet']], max_order=3, smooth=True, and the BERTScore for these predictions: predictions=['the sun is shining brightly', 'the sun is shining in the sky'], references=[['the sun is shining brightly in the sky', 'the sun is shining brightly today'], ['the sun is shining in the sky and the dog is happy', 'the sun is shining in the sky and the dog is sad']], lang='en', idf=True?",
    "answer": "Action: rouge\nAction Input: {\n  \"predictions\": [\n    \"the cat is very happy\",\n    \"the dog is very sleepy\"\n  ],\n  \"references\": [\n    [\n      \"the cat is very happy today\",\n      \"the cat is very happy now\"\n    ],\n    [\n      \"the dog is very sleepy now\",\n      \"the dog is very sleepy yesterday\"\n    ]\n  ],\n  \"rouge_types\": [\n    \"rouge1\",\n    \"rouge2\"\n  ],\n  \"use_aggregator\": true,\n  \"use_stemmer\": false\n}\n\nAction: bleu\nAction Input: {\n  \"predictions\": [\n    \"the house is blue\",\n    \"the apple is red\"\n  ],\n  \"references\": [\n    [\n      \"the house is blue and big\",\n      \"the house is old\"\n    ],\n    [\n      \"the apple is red and juicy\",\n      \"the apple is sweet\"\n    ]\n  ],\n  \"max_order\": 3,\n  \"smooth\": true\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"the sun is shining brightly\",\n    \"the sun is shining in the sky\"\n  ],\n  \"references\": [\n    [\n      \"the sun is shining brightly in the sky\",\n      \"the sun is shining brightly today\"\n    ],\n    [\n      \"the sun is shining in the sky and the dog is happy\",\n      \"the sun is shining in the sky and the dog is sad\"\n    ]\n  ],\n  \"lang\": \"en\",\n  \"idf\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bertscore, bleu\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bertscore\n  Description: BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.\n  Required parameters:\n    - predictions: The generated sentences.\n    - references: The reference sentences.\n    - lang: The language of the input text (e.g., 'en' for English).\n  Optional parameters:\n    - idf: Whether to use inverse document frequency reweighting (default: false).\n\nMetric name: bleu\n  Description: BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of *N* machine translation by comparing a candidate translation against *N* reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.\n  Required parameters:\n    - predictions: List of *N* translations to score\n    - references: list of *N* reference translations or a list of *N* lists of reference translations.\n  Optional parameters:\n    - max_order: Maximum n-gram order to consider (default: 4).\n    - smooth: Whether to apply smoothing (default: false).\n\n\n\n### User Query: \nWhat are the BLEU and BERTScore of [\"The quick brown fox\", \"jumps over the lazy dog\"] against [[\"The quick brown fox\", \"jumps over the sleepy dog\"], [\"The quick brown fox\", \"leaps over the lazy dog\"]], with BERTScore using idf reweighting and BLEU max_order 4 and smoothing?",
    "answer": "Action: bleu\nAction Input: {\n  \"predictions\": [\n    \"The quick brown fox\",\n    \"jumps over the lazy dog\"\n  ],\n  \"references\": [\n    [\n      \"The quick brown fox\",\n      \"jumps over the sleepy dog\"\n    ],\n    [\n      \"The quick brown fox\",\n      \"leaps over the lazy dog\"\n    ]\n  ],\n  \"max_order\": 4,\n  \"smooth\": true\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"The quick brown fox\",\n    \"jumps over the lazy dog\"\n  ],\n  \"references\": [\n    [\n      \"The quick brown fox\",\n      \"jumps over the sleepy dog\"\n    ],\n    [\n      \"The quick brown fox\",\n      \"leaps over the lazy dog\"\n    ]\n  ],\n  \"lang\": \"en\",\n  \"idf\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bleu\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bleu\n  Description: BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of *N* machine translation by comparing a candidate translation against *N* reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.\n  Required parameters:\n    - predictions: List of *N* translations to score\n    - references: list of *N* reference translations or a list of *N* lists of reference translations.\n  Optional parameters:\n    - max_order: Maximum n-gram order to consider (default: 4).\n    - smooth: Whether to apply smoothing (default: false).\n\n\n\n### User Query: \nWhat are the BLEU scores for the following pairs of texts: predictions = ['the cat sat on the mat', 'a quick brown fox'], references = [['the cat is sitting on the mat', 'the cat is laying on the mat'], ['a fast brown fox jumps over the lazy dog', 'a fast brownish fox jumps over the sleepy dog']], max_order=4, smooth=True?",
    "answer": "Action: bleu\nAction Input: {\n  \"predictions\": [\n    \"the cat sat on the mat\",\n    \"a quick brown fox\"\n  ],\n  \"references\": [\n    [\n      \"the cat is sitting on the mat\",\n      \"the cat is laying on the mat\"\n    ],\n    [\n      \"a fast brown fox jumps over the lazy dog\",\n      \"a fast brownish fox jumps over the sleepy dog\"\n    ]\n  ],\n  \"max_order\": 4,\n  \"smooth\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bertscore, recall, f1, bleu, rouge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bertscore\n  Description: BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.\n  Required parameters:\n    - predictions: The generated sentences.\n    - references: The reference sentences.\n    - lang: The language of the input text (e.g., 'en' for English).\n  Optional parameters:\n    - idf: Whether to use inverse document frequency reweighting (default: false).\n\nMetric name: recall\n  Description: Recall measures the proportion of actual positive instances that are correctly identified (Recall = TP / (TP + FN)), indicating the model's sensitivity.\n  Required parameters:\n    - predictions: Predicted labels.\n    - references: Ground Truth labels.\n  Optional parameters:\n    - average: This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data (e.g., 'binary', 'macro', 'micro', 'weighted', 'samples').\n    - labels: The set of labels to include when average is not set to 'binary', and the order of the labels if average is None.\n    - pos_label: The class to be considered the positive class, in the case where average is set to binary.\n    - sample_weight: Sample weights.\n\nMetric name: f1\n  Description: The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.\n  Required parameters:\n    - predictions: The *N* predicted labels.\n    - references: The *N* true labels.\n  Optional parameters:\n    - average: The averaging method to use (e.g., 'macro', 'micro', 'weighted').\n\nMetric name: bleu\n  Description: BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of *N* machine translation by comparing a candidate translation against *N* reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.\n  Required parameters:\n    - predictions: List of *N* translations to score\n    - references: list of *N* reference translations or a list of *N* lists of reference translations.\n  Optional parameters:\n    - max_order: Maximum n-gram order to consider (default: 4).\n    - smooth: Whether to apply smoothing (default: false).\n\nMetric name: rouge\n  Description: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.\n  Required parameters:\n    - predictions: list of *N* predictions to score. Each prediction should be a string with tokens separated by spaces\n    - references: list of *N* references (one for prediction) or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.\n  Optional parameters:\n    - rouge_types: Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).\n    - use_aggregator: If true, returns aggregated scores; if false, returns individual scores for each prediction-reference pair. Defaults to true.\n    - use_stemmer: If true, applies a Porter stemmer to normalize words before comparison. Defaults to false.\n\n\n\n### User Query: \nWhat are the ROUGE and BLEU scores of [\"The cat is very happy\", \"The dog is very sad\"], the BERTScore with idf reweighting of [\"The cat is very happy\", \"The dog is very sad\"] against [[\"The cat is extremely happy\", \"The dog is very sad\"], [\"The cat is quite happy\", \"The dog is not sad\"]], and the BERTScore precision, recall, and F1 scores of [\"The cat is playing with a ball\", \"The dog is running quickly\"] against [[\"The cat is playing with a toy\", \"The dog is running fast\"], [\"The cat is playing with a toy\", \"The dog is running slowly\"], [\"The cat is playing with a toy\", \"The dog is running slowly\"]], with BLEU max_order 2 and smoothing?",
    "answer": "Action: rouge\nAction Input: {\n  \"predictions\": [\n    \"The cat is very happy\",\n    \"The dog is very sad\"\n  ],\n  \"references\": [\n    [\n      \"The cat is extremely happy\",\n      \"The dog is very sad\"\n    ],\n    [\n      \"The cat is quite happy\",\n      \"The dog is not sad\"\n    ]\n  ],\n  \"rouge_types\": [\n    \"rouge1\",\n    \"rouge2\",\n    \"rougeL\",\n    \"rougeLsum\"\n  ],\n  \"use_aggregator\": true,\n  \"use_stemmer\": false\n}\n\nAction: bleu\nAction Input: {\n  \"predictions\": [\n    \"The cat is very happy\",\n    \"The dog is very sad\"\n  ],\n  \"references\": [\n    [\n      \"The cat is extremely happy\",\n      \"The dog is very sad\"\n    ],\n    [\n      \"The cat is quite happy\",\n      \"The dog is not sad\"\n    ]\n  ],\n  \"max_order\": 2,\n  \"smooth\": true\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"The cat is very happy\",\n    \"The dog is very sad\"\n  ],\n  \"references\": [\n    [\n      \"The cat is extremely happy\",\n      \"The dog is very sad\"\n    ],\n    [\n      \"The cat is quite happy\",\n      \"The dog is not sad\"\n    ]\n  ],\n  \"lang\": \"en\",\n  \"idf\": true\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"The cat is playing with a ball\",\n    \"The dog is running quickly\"\n  ],\n  \"references\": [\n    [\n      \"The cat is playing with a toy\",\n      \"The dog is running fast\"\n    ],\n    [\n      \"The cat is playing with a toy\",\n      \"The dog is running slowly\"\n    ]\n  ],\n  \"lang\": \"en\",\n  \"idf\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: rouge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: rouge\n  Description: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.\n  Required parameters:\n    - predictions: list of *N* predictions to score. Each prediction should be a string with tokens separated by spaces\n    - references: list of *N* references (one for prediction) or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.\n  Optional parameters:\n    - rouge_types: Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).\n    - use_aggregator: If true, returns aggregated scores; if false, returns individual scores for each prediction-reference pair. Defaults to true.\n    - use_stemmer: If true, applies a Porter stemmer to normalize words before comparison. Defaults to false.\n\n\n\n### User Query: \nWhat are the ROUGE scores for the following texts: \"The cat is sleeping on the mat\", \"The dog is barking loudly outside\", \"The sun was shining in the clear blue sky\" with \"The cat is sleeping on the mat\" and \"The dog is barking loudly outside\" and \"The sun was shining brightly in the clear blue sky\" as references?",
    "answer": "Action: rouge\nAction Input: {\n  \"predictions\": [\n    \"The cat is sleeping on the mat\",\n    \"The dog is barking loudly outside\",\n    \"The sun was shining in the clear blue sky\"\n  ],\n  \"references\": [\n    \"The cat is sleeping on the mat\",\n    \"The dog is barking loudly outside\",\n    \"The sun was shining brightly in the clear blue sky\"\n  ]\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"The cat is sleeping on the mat\",\n    \"The dog is barking loudly outside\",\n    \"The sun was shining in the clear blue sky\"\n  ],\n  \"references\": [\n    \"The cat is sleeping on the mat\",\n    \"The dog is barking loudly outside\",\n    \"The sun was shining brightly in the clear blue sky\"\n  ],\n  \"lang\": \"en\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bertscore, bleu, rouge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bertscore\n  Description: BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.\n  Required parameters:\n    - predictions: The generated sentences.\n    - references: The reference sentences.\n    - lang: The language of the input text (e.g., 'en' for English).\n  Optional parameters:\n    - idf: Whether to use inverse document frequency reweighting (default: false).\n\nMetric name: bleu\n  Description: BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of *N* machine translation by comparing a candidate translation against *N* reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.\n  Required parameters:\n    - predictions: List of *N* translations to score\n    - references: list of *N* reference translations or a list of *N* lists of reference translations.\n  Optional parameters:\n    - max_order: Maximum n-gram order to consider (default: 4).\n    - smooth: Whether to apply smoothing (default: false).\n\nMetric name: rouge\n  Description: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.\n  Required parameters:\n    - predictions: list of *N* predictions to score. Each prediction should be a string with tokens separated by spaces\n    - references: list of *N* references (one for prediction) or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.\n  Optional parameters:\n    - rouge_types: Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).\n    - use_aggregator: If true, returns aggregated scores; if false, returns individual scores for each prediction-reference pair. Defaults to true.\n    - use_stemmer: If true, applies a Porter stemmer to normalize words before comparison. Defaults to false.\n\n\n\n### User Query: \nWhat are the ROUGE and BLEU scores for the following predictions and references, including rouge1 and rougeL, and return individual scores for each prediction-reference pair, for the following predictions and references with multiple references per prediction, and also calculate the BERTScore with and without inverse document frequency reweighting, for the following predictions and references: predictions=['The cat is very happy', 'The dog is very sad'], references=[['The cat is happy', 'the dog is sad'], ['The cat is very happy', 'the dog is very sad', 'the cat is sad']], predictions=['The quick brown fox is very fast', 'jumps over the lazy dog'], references=[['The quick brown fox', 'leaps over the lazy dog'], ['The quick brown fox is very fast', 'jumps over the lazy dog', 'leaps over the lazy dog']], predictions=['The sun is shining brightly', 'the dog is running quickly'], references=[['The sun is shining', 'the dog is running'], ['The sun is shining brightly', 'the dog is running quickly', 'the sun is shining']], lang='en', max_order=4, smooth=False, idf=False, idf=True",
    "answer": "Action: rouge\nAction Input: {\n  \"predictions\": [\n    \"The cat is very happy\",\n    \"The dog is very sad\"\n  ],\n  \"references\": [\n    [\n      \"The cat is happy\",\n      \"the dog is sad\"\n    ],\n    [\n      \"The cat is very happy\",\n      \"the dog is very sad\",\n      \"the cat is sad\"\n    ]\n  ],\n  \"rouge_types\": [\n    \"rouge1\",\n    \"rougeL\"\n  ],\n  \"use_aggregator\": false,\n  \"use_stemmer\": false\n}\n\nAction: bleu\nAction Input: {\n  \"predictions\": [\n    \"The quick brown fox is very fast\",\n    \"jumps over the lazy dog\"\n  ],\n  \"references\": [\n    [\n      \"The quick brown fox\",\n      \"leaps over the lazy dog\"\n    ],\n    [\n      \"The quick brown fox is very fast\",\n      \"jumps over the lazy dog\",\n      \"leaps over the lazy dog\"\n    ]\n  ],\n  \"max_order\": 4,\n  \"smooth\": false\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"The quick brown fox is very fast\",\n    \"jumps over the lazy dog\"\n  ],\n  \"references\": [\n    [\n      \"The quick brown fox\",\n      \"leaps over the lazy dog\"\n    ],\n    [\n      \"The quick brown fox is very fast\",\n      \"jumps over the lazy dog\",\n      \"leaps over the lazy dog\"\n    ]\n  ],\n  \"lang\": \"en\",\n  \"idf\": true\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"The quick brown fox is very fast\",\n    \"jumps over the lazy dog\"\n  ],\n  \"references\": [\n    [\n      \"The quick brown fox\",\n      \"leaps over the lazy dog\"\n    ],\n    [\n      \"The quick brown fox is very fast\",\n      \"jumps over the lazy dog\",\n      \"leaps over the lazy dog\"\n    ]\n  ],\n  \"lang\": \"en\",\n  \"idf\": false\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bertscore, rouge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bertscore\n  Description: BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.\n  Required parameters:\n    - predictions: The generated sentences.\n    - references: The reference sentences.\n    - lang: The language of the input text (e.g., 'en' for English).\n  Optional parameters:\n    - idf: Whether to use inverse document frequency reweighting (default: false).\n\nMetric name: rouge\n  Description: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.\n  Required parameters:\n    - predictions: list of *N* predictions to score. Each prediction should be a string with tokens separated by spaces\n    - references: list of *N* references (one for prediction) or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.\n  Optional parameters:\n    - rouge_types: Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).\n    - use_aggregator: If true, returns aggregated scores; if false, returns individual scores for each prediction-reference pair. Defaults to true.\n    - use_stemmer: If true, applies a Porter stemmer to normalize words before comparison. Defaults to false.\n\n\n\n### User Query: \nWhat are the ROUGE and BERTScore of the following pairs of texts: predictions=['The new smartphone has a great camera', 'The latest smartphone release is very exciting']; references=[['The new smartphone has a great camera and a long battery life', 'The latest smartphone release has received overwhelmingly positive reviews'], ['The new smartphone has a good camera but a small battery', 'The latest smartphone release is a disappointment for its price'], ['The new smartphone has a great design but a small battery', 'The latest smartphone release is a disappointment for its price']]; rouge_types=['rouge1', 'rougeLsum']; idf=true; lang=en?",
    "answer": "Action: rouge\nAction Input: {\n  \"predictions\": [\n    \"The new smartphone has a great camera\",\n    \"The latest smartphone release is very exciting\"\n  ],\n  \"references\": [\n    [\n      \"The new smartphone has a great camera and a long battery life\",\n      \"The latest smartphone release has received overwhelmingly positive reviews\"\n    ],\n    [\n      \"The new smartphone has a good camera but a small battery\",\n      \"The latest smartphone release is a disappointment for its price\"\n    ]\n  ],\n  \"rouge_types\": [\n    \"rouge1\",\n    \"rougeLsum\"\n  ],\n  \"use_aggregator\": true,\n  \"use_stemmer\": false\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bertscore, bleu, rouge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bertscore\n  Description: BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.\n  Required parameters:\n    - predictions: The generated sentences.\n    - references: The reference sentences.\n    - lang: The language of the input text (e.g., 'en' for English).\n  Optional parameters:\n    - idf: Whether to use inverse document frequency reweighting (default: false).\n\nMetric name: bleu\n  Description: BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of *N* machine translation by comparing a candidate translation against *N* reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.\n  Required parameters:\n    - predictions: List of *N* translations to score\n    - references: list of *N* reference translations or a list of *N* lists of reference translations.\n  Optional parameters:\n    - max_order: Maximum n-gram order to consider (default: 4).\n    - smooth: Whether to apply smoothing (default: false).\n\nMetric name: rouge\n  Description: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.\n  Required parameters:\n    - predictions: list of *N* predictions to score. Each prediction should be a string with tokens separated by spaces\n    - references: list of *N* references (one for prediction) or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.\n  Optional parameters:\n    - rouge_types: Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).\n    - use_aggregator: If true, returns aggregated scores; if false, returns individual scores for each prediction-reference pair. Defaults to true.\n    - use_stemmer: If true, applies a Porter stemmer to normalize words before comparison. Defaults to false.\n\n\n\n### User Query: \nWhat are the ROUGE scores of the following summaries: ['the cat sat on the mat', 'the dog is happy'] against ['the cat sat on the mat', 'the dog is happy'], and the BLEU scores of the translations ['the cat sat on the mat', 'a quick brown fox'] against ['the cat is sitting on the mat', 'the cat is laying on the mat'], and the BERTScore similarity with inverse document frequency reweighting between ['The quick brown fox', 'jumps over the lazy dog'] and ['The quick brown fox', 'leaps over the lazy dog'] in English?",
    "answer": "Action: bertscore\nAction Input: {\n  \"predictions\": [\n    \"The quick brown fox\",\n    \"jumps over the lazy dog\"\n  ],\n  \"references\": [\n    \"The quick brown fox\",\n    \"leaps over the lazy dog\"\n  ],\n  \"idf\": true,\n  \"lang\": \"en\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bertscore, bleu, rouge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bertscore\n  Description: BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.\n  Required parameters:\n    - predictions: The generated sentences.\n    - references: The reference sentences.\n    - lang: The language of the input text (e.g., 'en' for English).\n  Optional parameters:\n    - idf: Whether to use inverse document frequency reweighting (default: false).\n\nMetric name: bleu\n  Description: BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of *N* machine translation by comparing a candidate translation against *N* reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.\n  Required parameters:\n    - predictions: List of *N* translations to score\n    - references: list of *N* reference translations or a list of *N* lists of reference translations.\n  Optional parameters:\n    - max_order: Maximum n-gram order to consider (default: 4).\n    - smooth: Whether to apply smoothing (default: false).\n\nMetric name: rouge\n  Description: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.\n  Required parameters:\n    - predictions: list of *N* predictions to score. Each prediction should be a string with tokens separated by spaces\n    - references: list of *N* references (one for prediction) or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.\n  Optional parameters:\n    - rouge_types: Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).\n    - use_aggregator: If true, returns aggregated scores; if false, returns individual scores for each prediction-reference pair. Defaults to true.\n    - use_stemmer: If true, applies a Porter stemmer to normalize words before comparison. Defaults to false.\n\n\n\n### User Query: \nWhat are the ROUGE scores for predictions=['the sun is shining in the sky', 'the sun is shining brightly'], references=[['the sun is shining in the sky and the dog is happy', 'the sun is shining brightly in the sky'], ['the sun is shining in the sky', 'the sun is shining brightly in the sky and the dog is sad']], rouge_types=['rouge1', 'rouge2'], use_aggregator=True, use_stemmer=False, and the BLEU scores for these predictions: predictions=['the sun is shining in the sky', 'the sun is shining brightly'], references=[['the sun is shining in the sky and the dog is happy', 'the sun is shining brightly in the sky'], ['the sun is shining in the sky', 'the sun is shining brightly in the sky and the dog is sad']], max_order=2, smooth=True, and the BERTScore for these predictions: predictions=['the sun is shining in the sky', 'the sun is shining brightly in the sky'], references=[['the sun is shining in the sky and the dog is happy', 'the sun is shining brightly in the sky'], ['the sun is shining in the sky', 'the sun is shining brightly in the sky and the dog is sad']], lang='en', idf=True?",
    "answer": "Action: rouge\nAction Input: {\n  \"predictions\": [\n    \"the sun is shining in the sky\",\n    \"the sun is shining brightly\"\n  ],\n  \"references\": [\n    [\n      \"the sun is shining in the sky and the dog is happy\",\n      \"the sun is shining brightly in the sky\"\n    ],\n    [\n      \"the sun is shining in the sky\",\n      \"the sun is shining brightly in the sky and the dog is sad\"\n    ]\n  ],\n  \"rouge_types\": [\n    \"rouge1\",\n    \"rouge2\"\n  ],\n  \"use_aggregator\": true,\n  \"use_stemmer\": false\n}\n\nAction: bleu\nAction Input: {\n  \"predictions\": [\n    \"the sun is shining in the sky\",\n    \"the sun is shining brightly\"\n  ],\n  \"references\": [\n    [\n      \"the sun is shining in the sky and the dog is happy\",\n      \"the sun is shining brightly in the sky\"\n    ],\n    [\n      \"the sun is shining in the sky\",\n      \"the sun is shining brightly in the sky and the dog is sad\"\n    ]\n  ],\n  \"max_order\": 2,\n  \"smooth\": true\n}\n\nAction: bertscore\nAction Input: {\n  \"predictions\": [\n    \"the sun is shining in the sky\",\n    \"the sun is shining brightly in the sky\"\n  ],\n  \"references\": [\n    [\n      \"the sun is shining in the sky and the dog is happy\",\n      \"the sun is shining brightly in the sky\"\n    ],\n    [\n      \"the sun is shining in the sky\",\n      \"the sun is shining brightly in the sky and the dog is sad\"\n    ]\n  ],\n  \"lang\": \"en\",\n  \"idf\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: accuracy\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: accuracy\n  Description: Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.\n  Required parameters:\n    - predictions: Predicted labels\n    - references: Ground truth labels.\n  Optional parameters:\n    - normalize: If set to false, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples\n    - sample_weight: Predicted labels\n\n\n\n### User Query: \nWhat are the accuracy scores of models with predictions [1, 0, 1, 1] and references [1, 1, 0, 0] when normalize is False and sample_weight is [0.5, 0.5]?",
    "answer": "Action: accuracy\nAction Input: {\n  \"predictions\": [\n    1,\n    0,\n    1\n  ],\n  \"references\": [\n    1,\n    1,\n    0\n  ],\n  \"normalize\": false,\n  \"sample_weight\": [\n    0.5,\n    0.5,\n    0\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: llm_judge\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: llm_judge\n  Description: LLM Judge uses a language model's own evaluation capabilities to assess the quality of a candidate text from multiple perspectives. Given a candidate text, a set of quality criteria, and optionally reference texts, the model returns a score (on a defined scale) and, if requested, a detailed explanation of its judgment.\n  Required parameters:\n    - candidate_texts: The texts whose quality is to be evaluated.\n    - quality_criteria: A list of aspects to evaluate (coherence, creativity, relevance, fluency).\n    - scale_max: The maximum value on the evaluation scale (e.g., 10 means scores will be in the range 0-10).\n  Optional parameters:\n    - explanation_required: If true, the LLM should provide a textual explanation along with the score.\n    - evaluation_type: The type of evaluation to perform (e.g., 'numeric', 'binary', 'qualitative'). Defaults to 'numeric'.\n    - prompt_template: A custom prompt template to guide the LLM's evaluation process, if desired.\n\n\n\n### User Query: \nWhat are the scores for evaluating the quality of these six creative texts: \"The world is a beautiful place full of wonder\", \"A single flower can brighten up a room\", \"The sound of a gentle stream is calming\", \"The taste of a ripe strawberry is a summer treat\", \"The feeling of sand between your toes on a hot summer day is refreshing\", \"The soft glow of candles on a quiet night is peaceful\", using the criteria of coherence, creativity, and relevance, on a scale of 10, with detailed explanation, using a binary evaluation type, and a custom prompt template \"Please assess the texts based on their ability to evoke a sense of appreciation for life's simple pleasures\"?",
    "answer": "Action: llm_judge\nAction Input: {\n  \"candidate_texts\": [\n    \"The world is a beautiful place full of wonder\",\n    \"A single flower can brighten up a room\",\n    \"The sound of a gentle stream is calming\",\n    \"The taste of a ripe strawberry is a summer treat\",\n    \"The feeling of sand between your toes on a hot summer day is refreshing\",\n    \"The soft glow of candles on a quiet night is peaceful\"\n  ],\n  \"quality_criteria\": [\n    \"coherence\",\n    \"creativity\",\n    \"relevance\"\n  ],\n  \"scale_max\": 10,\n  \"explanation_required\": true,\n  \"evaluation_type\": \"binary\",\n  \"prompt_template\": \"Please assess the texts based on their ability to evoke a sense of appreciation for life's simple pleasures\"\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: exact_match\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: exact_match\n  Description: Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.\n  Required parameters:\n    - predictions: List of predicted texts.\n    - references: List of reference texts.\n  Optional parameters:\n    - regexes_to_ignore: Regex expressions of characters to ignore when calculating the exact matches.\n    - ignore_case: If true, turns everything to lowercase so that capitalization differences are ignored.\n    - ignore_numbers (bool): If true, removes all digits before comparing strings\n    - ignore_punctuation (bool): If true, removes punctuation before comparing strings.\n\n\n\n### User Query: \nWhat is the exact match percentage between the predicted texts ['Lyon', 'Zurich', 'Geneva'] and the reference texts ['Lyon', 'Zurich', 'Geneva'], ignoring numbers, ignoring punctuation, treating everything as lowercase, ignoring the characters 'e' and 'o', and with regexes_to_ignore = ['\\d', '\\$']?",
    "answer": "Action: exact_match\nAction Input: {\n  \"predictions\": [\n    \"Lyon\",\n    \"Zurich\",\n    \"Geneva\"\n  ],\n  \"references\": [\n    \"Lyon\",\n    \"Zurich\",\n    \"Geneva\"\n  ],\n  \"ignore_numbers\": true,\n  \"ignore_punctuation\": true,\n  \"ignore_case\": true,\n  \"regexes_to_ignore\": [\n    \"\\\\d\",\n    \"\\\\$\"\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: perplexity\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: perplexity\n  Description: Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.\n  Required parameters:\n    - predictions: input text, where each separate text snippet is one list entry.\n    - model_id: model_id (str): model used for calculating Perplexity. NOTE: Perplexity can only be calculated for causal language models, such as gpt2, causal variations of bert, causal versions of t5. (e.g., 'gpt2').\n  Optional parameters:\n    - batch_size: the batch size to run texts through the model.\n    - add_start_token: whether to add the start token to the texts, so the perplexity can include the probability of the first word.\n\n\n\n### User Query: \nWhat is the perplexity of the following two groups of texts: \"The morning dew glistens on the grass.\", \"The sun rises over the horizon.\", \"The birds sing their sweet melodies.\", \"The world awakens from its slumber.\", \"The stars fade into the morning light.\" and \"The city comes alive with energy.\", \"The sounds of traffic fill the air.\", \"The people rush to and fro.\", \"The world is in constant motion.\" using the gpt2 model with a batch size of 32 and start tokens added, and compare it with the perplexity of \"The forest is dark and eerie.\", \"The trees loom above me.\", \"The wind howls through the branches.\", \"The world is full of secrets.\" and \"The flowers bloom in the darkness.\", \"The baby laughs at the shadows.\", \"The kids play in the moonlight.\", \"The world is a mysterious place.\"",
    "answer": "Action: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The morning dew glistens on the grass.\",\n    \"The sun rises over the horizon.\",\n    \"The birds sing their sweet melodies.\",\n    \"The world awakens from its slumber.\",\n    \"The stars fade into the morning light.\",\n    \"The city comes alive with energy.\",\n    \"The sounds of traffic fill the air.\",\n    \"The people rush to and fro.\",\n    \"The world is in constant motion.\"\n  ],\n  \"model_id\": \"gpt2\",\n  \"batch_size\": 32,\n  \"add_start_token\": true\n}\n\nAction: perplexity\nAction Input: {\n  \"predictions\": [\n    \"The forest is dark and eerie.\",\n    \"The trees loom above me.\",\n    \"The wind howls through the branches.\",\n    \"The world is full of secrets.\",\n    \"The flowers bloom in the darkness.\",\n    \"The baby laughs at the shadows.\",\n    \"The kids play in the moonlight.\",\n    \"The world is a mysterious place.\"\n  ],\n  \"model_id\": \"gpt2\",\n  \"batch_size\": 32,\n  \"add_start_token\": true\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: accuracy\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: accuracy\n  Description: Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.\n  Required parameters:\n    - predictions: Predicted labels\n    - references: Ground truth labels.\n  Optional parameters:\n    - normalize: If set to false, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples\n    - sample_weight: Predicted labels\n\n\n\n### User Query: \nWhat is the accuracy of the model when the predicted labels are [0, 1, 1, 0, 1] and the ground truth labels are [0, 0, 1, 0, 1]?",
    "answer": "Action: accuracy\nAction Input: {\n  \"predictions\": [\n    0,\n    1,\n    1,\n    0,\n    1\n  ],\n  \"references\": [\n    0,\n    0,\n    1,\n    0,\n    1\n  ]\n}"
  },
  {
    "system": "You are a bot that creates and responds to evaluation queries.",
    "query": "### Instructions: Try to respond to the query using the available evaluation metrics.\n\nThe format you use for the evaluation metric is by specifying 1) Action: the name of the metric you want to call 2) Action Input: the input parameters for the metric in a JSON string format such that predictions and references have the same number of entries in their lists. The result of the metric call will be returned starting with \"Evaluation Result\". *If the query requires multiple metrics call, then provide multiple Action and Action Input couples*\n\nReminder:\n1) The only values that should follow \"Action:\" are: bleu\n2) Use the following JSON string format for the evaluation arguments of each metric.\n\nAction Input:\n\\{\n    \"key_1\": \"value_1\",\n    ...\n    \"key_n\": \"value_n\",\n\\}\n\nAlways use the following answer format:\n \nAction: the evaluation metric name  \nAction Input: the input parameters for the metric function in JSON string format\n[...]\nAction: the Nth metric name\nAction input Nth related action input\n\n### Metrics documentation:\nMetric name: bleu\n  Description: BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of *N* machine translation by comparing a candidate translation against *N* reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.\n  Required parameters:\n    - predictions: List of *N* translations to score\n    - references: list of *N* reference translations or a list of *N* lists of reference translations.\n  Optional parameters:\n    - max_order: Maximum n-gram order to consider (default: 4).\n    - smooth: Whether to apply smoothing (default: false).\n\n\n\n### User Query: \nWhat are the BLEU scores for predictions=['the sun is shining brightly in the morning', 'the sun is shining in the sky with beautiful clouds'], references=[['the sun is shining brightly in the morning and it is warm', 'the sun is shining in the sky with big fluffy clouds'], ['the sun is shining in the morning and the birds are singing', 'the sun is shining in the sky with beautiful white clouds']], max_order=4, smooth=False?",
    "answer": "Action: bleu\nAction Input: {\n  \"predictions\": [\n    \"the sun is shining brightly in the morning\",\n    \"the sun is shining in the sky with beautiful clouds\"\n  ],\n  \"references\": [\n    [\n      \"the sun is shining brightly in the morning and it is warm\",\n      \"the sun is shining in the sky with big fluffy clouds\"\n    ],\n    [\n      \"the sun is shining in the morning and the birds are singing\",\n      \"the sun is shining in the sky with beautiful white clouds\"\n    ]\n  ],\n  \"max_order\": 4,\n  \"smooth\": false\n}"
  }
]