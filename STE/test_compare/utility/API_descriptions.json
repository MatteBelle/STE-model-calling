{
    "_meta": {
        "note": "All JSON examples must be in valid JSON format. Ensure correct use of double quotes, lists, and closing brackets."
    },
    "rouge": {
        "description": "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of STRING",
                "description": "list of *N* predictions to score. Each prediction should be a string with tokens separated by spaces",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of STRING",
                "description": "list of *N* references (one for prediction) or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "rouge_types",
                "type": "LIST of STRING",
                "description": "Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).",
                "default": "['rouge1', 'rouge2', 'rougeL', 'rougeLsum']"
            },
            {
                "name": "use_aggregator",
                "type": "BOOLEAN",
                "description": "If true, returns aggregated scores; if false, returns individual scores for each prediction-reference pair. Defaults to true.",
                "default": "true"
            },
            {
                "name": "use_stemmer",
                "type": "BOOLEAN",
                "description": "If true, applies a Porter stemmer to normalize words before comparison. Defaults to false.",
                "default": "false"
            }
        ],
        "example": {
            "predictions": [
                "the cat sat on the mat",
                "the quick brown fox"
            ],
            "references": [
                "the cat sat on the mat",
                "the quick brown fox jumps over the lazy dog"
            ]
        }
    },
    "bleu": {
        "description": "BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of *N* machine translation by comparing a candidate translation against *N* reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of STRING",
                "description": "List of *N* translations to score",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of STRING",
                "description": "list of *N* reference translations or a list of *N* lists of reference translations.",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "max_order",
                "type": "NUMBER",
                "description": "Maximum n-gram order to consider (default: 4).",
                "default": "4"
            },
            {
                "name": "smooth",
                "type": "BOOLEAN",
                "description": "Whether to apply smoothing (default: false).",
                "default": "false"
            }
        ],
        "example": {
            "predictions": [
                "the cat sat on the mat",
                "a quick brown fox"
            ],
            "references": [
                [
                    "the cat is sitting on the mat",
                    "the cat is laying on the mat"
                ],
                [
                    "a fast brown fox jumps over the lazy dog",
                    "a fast brownish fox jumps over the sleepy dog"
                ]
            ]
        }
    },
    "bertscore": {
        "description": "BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of STRING",
                "description": "The generated sentences.",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of STRING",
                "description": "The reference sentences.",
                "default": ""
            },
            {
                "name": "lang",
                "type": "STRING",
                "description": "The language of the input text (e.g., 'en' for English).",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "idf",
                "type": "BOOLEAN",
                "description": "Whether to use inverse document frequency reweighting (default: false).",
                "default": "false"
            }
        ],
        "example": {
            "predictions": [
                "The quick brown fox",
                "jumps over the lazy dog"
            ],
            "references": [
                "The quick brown fox",
                "leaps over the lazy dog"
            ],
            "lang": "en"
        }
    },
    "perplexity": {
        "description": "Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of STRING",
                "description": "input text, where each separate text snippet is one list entry.",
                "default": ""
            },
            {
                "name": "model_id",
                "type": "STRING",
                "description": "model_id (str): model used for calculating Perplexity. NOTE: Perplexity can only be calculated for causal language models, such as gpt2, causal variations of bert, causal versions of t5. (e.g., 'gpt2').",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "batch_size",
                "type": "INT",
                "description": "the batch size to run texts through the model.",
                "default": "16"
            },
            {
                "name": "add_start_token",
                "type": "BOOLEAN",
                "description": "whether to add the start token to the texts, so the perplexity can include the probability of the first word.",
                "default": "true"
            }
        ],
        "example": {
            "texts": [
                "The quick brown fox jumps over the lazy dog.",
                "An apple a day keeps the doctor away."
            ],
            "model_id": "gpt2"
        }
    },
    "accuracy": {
        "description": "Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of INT",
                "description": "Predicted labels",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of INT",
                "description": "Ground truth labels.",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "normalize",
                "type": "BOOLEAN",
                "description": "If set to false, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples",
                "default": "true"
            },
            {
                "name": "sample_weight",
                "type": "LIST of FLOAT",
                "description": "Predicted labels",
                "default": "None"
            }
        ],
        "example": {
            "predictions": [
                1,
                0,
                1,
                1
            ],
            "references": [
                1,
                1,
                1,
                0
            ]
        }
    },
    "exact_match": {
        "description": "Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of STRING",
                "description": "List of predicted texts.",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of STRING",
                "description": "List of reference texts.",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "regexes_to_ignore",
                "type": "LIST of STRING",
                "description": "Regex expressions of characters to ignore when calculating the exact matches.",
                "default": "None"
            },
            {
                "name": "ignore_case",
                "type": "BOOLEAN",
                "description": "If true, turns everything to lowercase so that capitalization differences are ignored.",
                "default": "false"
            },
            {
                "name": "ignore_numbers (bool)",
                "type": "BOOLEAN",
                "description": "If true, removes all digits before comparing strings",
                "default": "false"
            },
            {
                "name": "ignore_punctuation (bool)",
                "type": "BOOLEAN",
                "description": "If true, removes punctuation before comparing strings.",
                "default": "false"
            }
        ],
        "example": {
            "predictions": [
                "Paris",
                "London",
                "Berlin"
            ],
            "references": [
                "Paris",
                "London",
                "Rome"
            ]
        }
    },
    "recall": {
        "description": "Recall measures the proportion of actual positive instances that are correctly identified (Recall = TP / (TP + FN)), indicating the model's sensitivity.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of INT",
                "description": "Predicted labels.",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of INT",
                "description": "Ground Truth labels.",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "average",
                "type": "STRING",
                "description": "This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data (e.g., 'binary', 'macro', 'micro', 'weighted', 'samples').",
                "default": "binary"
            },
            {
                "name": "labels",
                "type": "LIST of INT",
                "description": "The set of labels to include when average is not set to 'binary', and the order of the labels if average is None.",
                "default": "None"
            },
            {
                "name": "pos_label",
                "type": "INT",
                "description": "The class to be considered the positive class, in the case where average is set to binary.",
                "default": "1"
            },
            {
                "name": "sample_weight",
                "type": "LIST of FLOAT",
                "description": "Sample weights.",
                "default": "None"
            }
        ],
        "example": {
            "predictions": [
                1,
                0,
                1,
                0
            ],
            "references": [
                1,
                1,
                1,
                0
            ]
        }
    },
    "f1": {
        "description": "The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of ANY",
                "description": "The *N* predicted labels.",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of ANY",
                "description": "The *N* true labels.",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "average",
                "type": "STRING",
                "description": "The averaging method to use (e.g., 'macro', 'micro', 'weighted').",
                "default": "macro"
            }
        ],
        "example": {
            "predictions": [
                1,
                0,
                1,
                0
            ],
            "references": [
                1,
                1,
                0,
                0
            ]
        }
    },
    "llm_judge": {
        "description": "LLM Judge uses a language model's own evaluation capabilities to assess the quality of a candidate text from multiple perspectives. Given a candidate text, a set of quality criteria, and optionally reference texts, the model returns a score (on a defined scale) and, if requested, a detailed explanation of its judgment.",
        "required_parameters": [
            {
                "name": "candidate_texts",
                "type": "LIST of STRING",
                "description": "The texts whose quality is to be evaluated.",
                "default": ""
            },
            {
                "name": "quality_criteria",
                "type": "LIST of STRING",
                "description": "A list of aspects to evaluate (coherence, creativity, relevance, fluency).",
                "default": ""
            },
            {
                "name": "scale_max",
                "type": "NUMBER",
                "description": "The maximum value on the evaluation scale (e.g., 10 means scores will be in the range 0-10).",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "explanation_required",
                "type": "BOOLEAN",
                "description": "If true, the LLM should provide a textual explanation along with the score.",
                "default": "false"
            },
            {
                "name": "evaluation_type",
                "type": "STRING",
                "description": "The type of evaluation to perform (e.g., 'numeric', 'binary', 'qualitative'). Defaults to 'numeric'.",
                "default": "numeric"
            },
            {
                "name": "prompt_template",
                "type": "STRING",
                "description": "A custom prompt template to guide the LLM's evaluation process, if desired.",
                "default": ""
            }
        ],
        "example": {
            "candidate_text": "The movie was breathtaking with its stunning visuals and unpredictable plot twists.",
            "quality_criteria": [
                "coherence",
                "creativity",
                "relevance"
            ],
            "scale_max": 10,
            "explanation_required": true,
            "evaluation_type": "numeric"
        }
    }
}