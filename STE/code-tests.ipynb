{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: \"assistant\n",
      "\n",
      "What is the perplexity of the gpt2 model for the following texts: ['The sun is shining brightly in the clear blue sky', 'The cat purrs contentedly on my lap', [SAMPLE_TEXTS] ]? \n",
      " Solved: Yes\n",
      "Query: \"assistant\n",
      "\n",
      "What is the accuracy of the model given the predictions [1, 0, 1, 1, 1, 0, 0, 1, 0, 1] and the references [1, 1, 0, 1, 1, 0, 0, 0, 1, 0]? \n",
      " Solved: No\n",
      "Query: \"What is the BLEU score for the translations [[\"hello\", \"world\"], [\"test\", \"example\"], [SAMPLE_TEXTS] ], [\"test\", \"example\"], [\"data\", \"more data\"]] against [[\"reference\", \"gold\"], [\"correct\", \"baseline\"]]?\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def trim_ltm_stm(ltm_list, placeholder=\"[...]\", max_length=3000, list_trim_threshold=30):\n",
    "    \"\"\"\n",
    "    Trims long text lists in the LTM entries and replaces them with a placeholder, \n",
    "    while preserving some examples for context. List of lists are trimmed only if too long.\n",
    "\n",
    "    Args:\n",
    "        ltm_list (list): List of strings containing the LTM queries.\n",
    "        placeholder (str): Placeholder text to replace long lists.\n",
    "        max_length (int): Max length of entire entry before truncation.\n",
    "        list_trim_threshold (int): Maximum number of characters a list can have before being trimmed.\n",
    "\n",
    "    Returns:\n",
    "        list: Trimmed LTM list.\n",
    "    \"\"\"\n",
    "    trimmed_ltm = []\n",
    "\n",
    "    for entry in ltm_list:\n",
    "        # Function to selectively trim long lists but keep part of the data\n",
    "        def trim_list(match):\n",
    "            content = match.group(0)  # The entire matched list\n",
    "            if len(content) > list_trim_threshold:\n",
    "                # Preserve the first few elements of the list while trimming the rest\n",
    "                partial_content = re.sub(r\"(\\[.*?, .*?)\\s*,\\s*.*?\\]\", rf\"\\1, {placeholder} ]\", content, flags=re.DOTALL)\n",
    "                return partial_content.replace(f\"[{placeholder},\", f\"[{placeholder}\")  # Cleanup format\n",
    "            return content  # If the list is short, return as is\n",
    "\n",
    "        # Function to handle nested lists (list of lists)\n",
    "        def trim_nested_list(match):\n",
    "            content = match.group(0)  # The entire matched nested list\n",
    "            if len(content) > list_trim_threshold:\n",
    "                # Preserve some nested elements while trimming the rest\n",
    "                partial_content = re.sub(r\"(\\[\\[.*?\\], \\[.*?\\])\\s*,\\s*.*?\\]\", rf\"\\1, {placeholder} ]\", content, flags=re.DOTALL)\n",
    "                return partial_content.replace(f\"[{placeholder},\", f\"[{placeholder}\")  # Cleanup format\n",
    "            return content  # If the nested list is short, return as is\n",
    "\n",
    "        # Trim only long nested lists (list of lists)\n",
    "        trimmed_entry = re.sub(r\"\\[\\s*(?:\\[[^\\]]*\\]\\s*,?\\s*)+\\]\", trim_nested_list, entry, flags=re.DOTALL)  \n",
    "\n",
    "        # Trim only long lists inside brackets\n",
    "        trimmed_entry = re.sub(r\"\\[.*?\\]\", trim_list, trimmed_entry, flags=re.DOTALL)  \n",
    "\n",
    "        # If the whole entry is still too long, truncate the text itself\n",
    "        if len(trimmed_entry) > max_length:\n",
    "            trimmed_entry = trimmed_entry[:max_length] + \" \" + placeholder\n",
    "\n",
    "        trimmed_ltm.append(trimmed_entry)\n",
    "\n",
    "    return trimmed_ltm\n",
    "\n",
    "# Example usage with your LTM list\n",
    "ltm_list = [\n",
    "    'Query: \"assistant\\n\\nWhat is the perplexity of the gpt2 model for the following texts: [\\'The sun is shining brightly in the clear blue sky\\', \\'The cat purrs contentedly on my lap\\', \\'The dog wags its tail with excitement\\', \\'The baby laughs at the silly clown\\', \\'The flowers bloom beautifully in the garden\\']? \\n Solved: Yes',\n",
    "    'Query: \"assistant\\n\\nWhat is the accuracy of the model given the predictions [1, 0, 1, 1, 1, 0, 0, 1, 0, 1] and the references [1, 1, 0, 1, 1, 0, 0, 0, 1, 0]? \\n Solved: No',\n",
    "    'Query: \"What is the BLEU score for the translations [[\"hello\", \"world\"], [\"test\", \"example\"], [\"test\", \"example\"], [\"test\", \"example\"], [\"data\", \"more data\"]] against [[\"reference\", \"gold\"], [\"correct\", \"baseline\"]]?\"'\n",
    "]\n",
    "\n",
    "trimmed_ltm = trim_ltm_stm(ltm_list)\n",
    "for entry in trimmed_ltm:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in file: 546701\n",
      "===== Suspicious JSON Fragment =====\n",
      "or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.\", \"default\": \"\"}], \"optional_param fox jumps over the lazy dog\\\"],\\n    \\\"rouge_types\\\": [\\\"rouge1\\\", \\\"rouge2\\\", \\\"rougeL\\\", \\\"rougeLsum\\\"],\\n    \\\"use_aggregator\\\": true,\\n    \\\"use_stemmer\\\": false\\n}\\n\\nFinal Answer: The ROUGE metric provides a good evaluation of the given query, but it may be beneficial to consider other metrics to get a more comprehensive understanding of the results.\"}, {\"role\": \"user\", \"content\": \"\\\"Do you think you successfully fulfilled this query in the end? Respond with \\\\\\\"Yes\\\\\\\" or \\\\\\\"No\\\\\\\".\\\"\"}, {\"role\": \"assistant\", \"content\": \"\\\"assistant\\n\\nYes\"}, {\"role\": \"user\", \"content\": \"Now you know a bit more about the available evaluation metrics. You can synthesize another user query to explore the metrics further and consolidate your understanding based on what you discovered. Again, just input the user query alone; do NOT solve the query for now.\\n\\nU\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"saved_results/data_dict_20250221_184302.json\"\n",
    "\n",
    "# Read the entire JSON file as a single string\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(f\"Total characters in file: {len(content)}\")\n",
    "\n",
    "# Define the problematic position\n",
    "error_position = 540793\n",
    "context_size = 500# Number of characters before and after the error\n",
    "\n",
    "# Ensure we don't go out of bounds\n",
    "start_index = max(0, error_position - context_size)\n",
    "end_index = min(len(content), error_position + context_size)\n",
    "\n",
    "# Print the suspect portion\n",
    "print(\"===== Suspicious JSON Fragment =====\")\n",
    "print(content[start_index:end_index])\n",
    "print(\"===================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ':' delimiter: line 1 column 540793 (char 540792)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load saved results\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_results/data_dict_20250221_184302.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m     data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Print all stored APIs\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPIs tested:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(data_dict\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/LLM/lib/python3.11/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/LLM/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/miniconda3/envs/LLM/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/LLM/lib/python3.11/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ':' delimiter: line 1 column 540793 (char 540792)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load saved results\n",
    "with open(\"saved_results/data_dict_20250221_184302.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "# Print all stored APIs\n",
    "print(\"APIs tested:\", list(data_dict.keys()))\n",
    "\n",
    "# View first session of a specific API\n",
    "api_name = \"accuracy\"\n",
    "print(\"First session for API:\", api_name)\n",
    "print(json.dumps(data_dict[api_name][0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [0, 1, 1, 0, 0, 1, 0, 1, 1, 0],\n",
       " 'references': [0, 1, 0, 1, 1, 0, 1, 0, 0, 1],\n",
       " 'normalize': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "js = '{\"predictions\": [0, 1, 1, 0, 0, 1, 0, 1, 1, 0], \"references\": [0, 1, 0, 1, 1, 0, 1, 0, 0, 1], \"normalize\": false}'\n",
    "json.loads(str(js).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.6354346056137057, 'precisions': [0.9545454545454546, 0.7894736842105263, 0.5625, 0.38461538461538464], 'brevity_penalty': 1.0, 'length_ratio': 1.0476190476190477, 'translation_length': 22, 'reference_length': 21}\n",
      "Error:  Either 'lang' (e.g. 'en') or 'model_type' (e.g. 'microsoft/deberta-xlarge-mnli') must be specified\n",
      "Error:  Perplexity._compute() missing 1 required positional argument: 'model_id'\n",
      "{'rouge1': 0.9079365079365079, 'rouge2': 0.735042735042735, 'rougeL': 0.9079365079365079, 'rougeLsum': 0.9079365079365079}\n",
      "Error:  invalid literal for int() with base 10: 'The sun is shining brightly in the sky'\n",
      "Error:  Predictions and/or references don't match the expected format.\n",
      "Expected format: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\n",
      "Input predictions: ['The sun is shining brightly in the sky', 'The cat is sleeping on the couch', 'The dog is running in the park'],\n",
      "Input references: [['The sun is shining in the sky', 'The cat is sleeping on the couch', 'The dog is running in the park'], ['The sun is shining in the clear blue sky', 'The cat is sleeping on the soft couch', 'The dog is running around the park'], ['The sun is shining in the clear blue sky', 'The cat is sleeping on the soft couch', 'The dog is running around the park']]\n",
      "Error:  invalid literal for int() with base 10: 'The sun is shining brightly in the sky'\n",
      "Error:  invalid literal for int() with base 10: 'The sun is shining brightly in the sky'\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "#\"predictions\": [\"The sun is shining brightly in the sky\", \"The cat is sleeping on the couch\", \"The dog is running in the park\"]\n",
    "#\"references\": [[\"The sun is shining in the sky\", \"The cat is sleeping on the couch\", \"The dog is running in the park\"], [\"The sun is shining in the clear blue sky\", \"The cat is sleeping on the soft couch\", \"The dog is running around the park\"]]\n",
    "#\"metric_name\": \"bleu\"\n",
    "predictions = [\"The sun is shining brightly in the sky\", \"The cat is sleeping on the couch\", \"The dog is running in the park\"]\n",
    "references = [[\"The sun is shining in the sky\", \"The cat is sleeping on the couch\", \"The dog is running in the park\"], [\"The sun is shining in the clear blue sky\", \"The cat is sleeping on the soft couch\", \"The dog is running around the park\"], [\"The sun is shining in the clear blue sky\", \"The cat is sleeping on the soft couch\", \"The dog is running around the park\"]]\n",
    "metrics = [  \n",
    "  \"bleu\",\n",
    "  \"bertscore\",\n",
    "  \"perplexity\",\n",
    "  \"rouge\",\n",
    "  \"accuracy\",\n",
    "  \"exact_match\",\n",
    "  \"recall\",\n",
    "  \"f1\"\n",
    "]\n",
    "for metric_name in metrics:\n",
    "    try :\n",
    "        args = {\n",
    "            \"predictions\": predictions,\n",
    "            \"references\": references\n",
    "        }\n",
    "        metric = evaluate.load(metric_name)\n",
    "        result = metric.compute(**args)\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bububu\n"
     ]
    }
   ],
   "source": [
    "    response_text = \"blablabla <|promptends|> bububu\"\n",
    "    parts = response_text.split(\"<|promptends|>\")\n",
    "    if parts:\n",
    "        response_text = parts[1].strip()\n",
    "        print(response_text)\n",
    "    else:\n",
    "        print(\"DEBUG ERROR: <|promptends|> not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970\n",
      "1988\n"
     ]
    }
   ],
   "source": [
    "print(len(\" [SYSTEM]: You are a helpful assistant.\\\n",
    "[USER]: You are an **expert** assisting in evaluation.   \\\n",
    "Your task is to **output a single, well-structured query** that a user might naturally ask when requesting a metric evaluation.  \\\n",
    "**Context:**\\\n",
    "API_name: exact_match\\\n",
    "Description: {'description': 'Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.', 'required_parameters': [{'name': 'predictions', 'type': 'LIST of STRING', 'description': 'TList of predicted texts.', 'default': ''}, {'name': 'references', 'type': 'LIST of STRING', 'description': 'List of reference texts.', 'default': ''}], 'optional_parameters': [{'name': 'regexes_to_ignore', 'type': 'LIST of STRING', 'description': 'Regex expressions of characters to ignore when calculating the exact matches.', 'default': 'None'}, {'name': 'ignore_case', 'type': 'BOOLEAN', 'description': 'If True, turns everything to lowercase so that capitalization differences are ignored.', 'default': 'False'}, {'name': 'ignore_numbers (bool)', 'type': 'BOOLEAN', 'description': 'If True, removes all digits before comparing strings', 'default': 'False'}, {'name': 'ignore_punctuation (bool)', 'type': 'BOOLEAN', 'description': 'If True, removes punctuation before comparing strings.', 'default': 'False'}], 'example': {'predictions': ['Paris', 'London', 'Berlin'], 'references': ['Paris', 'London', 'Rome']}}\\\n",
    "\\\n",
    "**Task Instructions:**  \\\n",
    "Generate **one** realistic user query.  \\\n",
    "The query should be **concise, natural, and human-like**.  \\\n",
    "The query should **only** request metric evaluation **for a set references and predictions**.  \\\n",
    "It should provide parameters.   \\\n",
    "The query should provide very creative, diverse and long references and predictions.   \\\n",
    "Do **not** add explanations, descriptions, or metadata.  \\\n",
    "Do **not** repeat yourself.  \\\n",
    "Do **not** format the query as JSON or a code block.  \\\n",
    "**Stop after outputting the query.**\\\n",
    "\\\n",
    "User Query:\\\n",
    "\"))\n",
    "\n",
    "print(len(\"[SYSTEM]: You are a helpful assistant.\\n[USER]: You are an **expert** assisting in evaluation.   \\nYour task is to **output a single, well-structured query** that a user might naturally ask when requesting a metric evaluation.  \\n**Context:**\\nAPI_name: exact_match\\nDescription: {'description': 'Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.', 'required_parameters': [{'name': 'predictions', 'type': 'LIST of STRING', 'description': 'TList of predicted texts.', 'default': ''}, {'name': 'references', 'type': 'LIST of STRING', 'description': 'List of reference texts.', 'default': ''}], 'optional_parameters': [{'name': 'regexes_to_ignore', 'type': 'LIST of STRING', 'description': 'Regex expressions of characters to ignore when calculating the exact matches.', 'default': 'None'}, {'name': 'ignore_case', 'type': 'BOOLEAN', 'description': 'If True, turns everything to lowercase so that capitalization differences are ignored.', 'default': 'False'}, {'name': 'ignore_numbers (bool)', 'type': 'BOOLEAN', 'description': 'If True, removes all digits before comparing strings', 'default': 'False'}, {'name': 'ignore_punctuation (bool)', 'type': 'BOOLEAN', 'description': 'If True, removes punctuation before comparing strings.', 'default': 'False'}], 'example': {'predictions': ['Paris', 'London', 'Berlin'], 'references': ['Paris', 'London', 'Rome']}}\\n\\n**Task Instructions:**  \\nGenerate **one** realistic user query.  \\nThe query should be **concise, natural, and human-like**.  \\nThe query should **only** request metric evaluation **for a set references and predictions**.  \\nIt should provide parameters.   \\nThe query should provide very creative, diverse and long references and predictions.   \\nDo **not** add explanations, descriptions, or metadata.  \\nDo **not** repeat yourself.  \\nDo **not** format the query as JSON or a code block.  \\n**Stop after outputting the query.**\\n\\nUser Query:\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected subgroup: classification_exact\n",
      "Metrics in subgroup: ['exact_match', 'recall', 'f1']\n",
      "Optional parameter flags:\n",
      "  exact_match: False\n",
      "  recall: False\n",
      "  f1: False\n"
     ]
    }
   ],
   "source": [
    "# Code to generate a random metric subgroup with optional flags for each metric.\n",
    "\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "def get_random_metric_subgroup_with_flags(json_path=\"tool_metadata/API_subgroups.json\"):\n",
    "    \"\"\"\n",
    "    Loads the metric subgroups from a JSON file, randomly selects one subgroup,\n",
    "    and for each metric in that subgroup, generates a boolean flag that is True\n",
    "    with 30% probability (and False with 70% probability).\n",
    "\n",
    "    Parameters:\n",
    "        json_path (str): Path to the JSON file containing the metric subgroups.\n",
    "\n",
    "    Returns:\n",
    "        parameters: A parametersionary with the following keys:\n",
    "            - \"name\": The name of the selected subgroup.\n",
    "            - \"metrics\": A list of metric names in the subgroup.\n",
    "            - \"optional_flags\": A parametersionary mapping each metric name to a boolean flag.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"Subgroups JSON file not found at: {json_path}\")\n",
    "    \n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        subgroups = json.load(f)\n",
    "    \n",
    "    # Randomly choose one subgroup from the parametersionary values.\n",
    "    chosen_subgroup = random.choice(list(subgroups.values()))\n",
    "    \n",
    "    # For each metric in the subgroup, assign a boolean flag (True with probability 0.3).\n",
    "    optional_flags = {metric: (random.random() < 0.3) for metric in chosen_subgroup[\"metrics\"]}\n",
    "    \n",
    "    return {\n",
    "        \"name\": chosen_subgroup[\"name\"],\n",
    "        \"metrics\": chosen_subgroup[\"metrics\"],\n",
    "        \"optional_flags\": optional_flags\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "#if __name__ == \"__main__\":\n",
    "subgroup_with_flags = get_random_metric_subgroup_with_flags()\n",
    "print(\"Selected subgroup:\", subgroup_with_flags[\"name\"])\n",
    "print(\"Metrics in subgroup:\", subgroup_with_flags[\"metrics\"])\n",
    "print(\"Optional parameter flags:\")\n",
    "for metric, flag in subgroup_with_flags[\"optional_flags\"].items():\n",
    "    print(f\"  {metric}: {flag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: nltk in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: absl-py in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (2.1.0)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: torch in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (4.48.1)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (1.15.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (1.6.1)\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: dill in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (0.28.1)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: click in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from rouge-score) (1.17.0)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: matplotlib in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from bert_score) (3.10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (19.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from matplotlib->bert_score) (4.55.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from matplotlib->bert_score) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from matplotlib->bert_score) (3.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=724072b1a4b5855afb55be76c4004fd54fc018c77e5bf782d26acb06ddf05e05\n",
      "  Stored in directory: /Users/a39328/Library/Caches/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: tabulate, portalocker, colorama, sacrebleu, rouge-score, bert_score\n",
      "Successfully installed bert_score-0.3.13 colorama-0.4.6 portalocker-3.1.1 rouge-score-0.1.2 sacrebleu-2.5.1 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate nltk absl-py rouge-score sacrebleu torch transformers numpy scipy scikit-learn bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating metric: rouge\n",
      "Results for rouge: {'rouge1': 0.5, 'rouge2': 0.33333333333333326, 'rougeL': 0.5, 'rougeLsum': 0.5}\n",
      "Evaluating metric: bleu\n",
      "Results for bleu: {'bleu': 0.3181877033696365, 'precisions': [0.6363636363636364, 0.4, 0.3333333333333333, 0.25], 'brevity_penalty': 0.8337529180751805, 'length_ratio': 0.8461538461538461, 'translation_length': 11, 'reference_length': 13}\n",
      "Evaluating metric: bertscore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bertscore: {'precision': [0.9666957259178162], 'recall': [0.9491630792617798], 'f1': [0.9578492045402527], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.48.1)'}\n",
      "Evaluating metric: perplexity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575dfe340dcc46b7a66e98f7bae3a237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for perplexity: {'perplexities': [14.477904319763184], 'mean_perplexity': 14.477904319763184}\n",
      "Evaluating metric: accuracy\n",
      "Results for accuracy: {'accuracy': 0.75}\n",
      "Evaluating metric: exact_match\n",
      "Results for exact_match: {'exact_match': 1.0}\n",
      "Evaluating metric: recall\n",
      "Results for recall: {'recall': 1.0}\n",
      "Evaluating metric: r_squared\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bf35ff7dd54f369227d9fae8c5788a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating r_squared: r_squared._compute() got an unexpected keyword argument 'squared'\n",
      "Evaluating metric: f1\n",
      "Results for f1: {'f1': 0.8}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load the API metrics from the JSON file.\n",
    "api_metrics_json_path = \"tool_metadata/API_list.json\"\n",
    "if not os.path.exists(api_metrics_json_path):\n",
    "    raise FileNotFoundError(f\"API metrics JSON file not found at: {api_metrics_json_path}\")\n",
    "\n",
    "with open(api_metrics_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    api_metrics = json.load(f)\n",
    "\n",
    "# Dictionary specifying the input format for each metric\n",
    "metric_inputs = {\n",
    "    \"rouge\": {\"predictions\": [\"Once upon a time, there was a brave hero.\"], \n",
    "              \"references\": [\"Once upon a time, a fearless warrior embarked on a journey.\"]},\n",
    "    \"bleu\": {\"predictions\": [\"Once upon a time, there was a brave hero.\"], \n",
    "             \"references\": [[\"Once upon a time, a fearless warrior embarked on a journey.\"]]},\n",
    "    \"bertscore\": {\"predictions\": [\"Once upon a time, there was a brave hero.\"], \n",
    "                  \"references\": [\"Once upon a time, a fearless warrior embarked on a journey.\"], \"lang\": \"en\"},\n",
    "    \"perplexity\": {\"predictions\": [\"Once upon a time, there was a brave hero.\"], \"model_id\": \"gpt2\"},\n",
    "    \"accuracy\": {\"predictions\": [1, 0, 1, 1], \"references\": [1, 0, 1, 0]},\n",
    "    \"exact_match\": {\"predictions\": [\"Once upon a time, there was a brave hero.\"], \n",
    "                    \"references\": [\"Once upon a time, there was a brave hero.\"]},\n",
    "    \"recall\": {\n",
    "        \"predictions\": [1, 0, 1, 1], \n",
    "        \"references\": [1, 0, 1, 0]  # Changed to numeric classification labels\n",
    "    },\n",
    "    \"f1\": {\n",
    "        \"predictions\": [1, 0, 1, 1], \n",
    "        \"references\": [1, 0, 1, 0]  # Changed to numeric classification labels\n",
    "    }\n",
    "}\n",
    "\n",
    "# Evaluate each metric in the API list.\n",
    "for metric_name in api_metrics:\n",
    "    if metric_name not in metric_inputs:\n",
    "        print(f\"Skipping {metric_name}: No input data provided.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Evaluating metric: {metric_name}\")\n",
    "\n",
    "    try:\n",
    "        # Load the metric\n",
    "        metric = evaluate.load(metric_name)\n",
    "\n",
    "        # Compute the metric\n",
    "        results = metric.compute(**metric_inputs[metric_name])\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Results for {metric_name}: {results}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {metric_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp =''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "this is a test\n"
     ]
    }
   ],
   "source": [
    "response = \"this is a test\"\n",
    "print(len(response))\n",
    "print(response[:len(response)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatch in the number of predictions (2) and references (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m references \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe cat is sitting on the mat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dog is sleeping on the bed\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Compute BLEU score\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mbleu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/LLM/lib/python3.11/site-packages/evaluate/module.py:455\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/LLM/lib/python3.11/site-packages/evaluate/module.py:546\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m     )\n\u001b[0;32m--> 546\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch in the number of predictions (2) and references (1)"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load the BLEU metric\n",
    "bleu = load(\"bleu\")\n",
    "# Example predictions and references\n",
    "predictions = [\"The cat is sitting on the mat\", \"The dog is sleeping on the bed\"]\n",
    "\n",
    "references = [[\"The cat is sitting on the mat\", \"The dog is sleeping on the bed\"]]\n",
    "\n",
    "# Compute BLEU score\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Display results\n",
    "print(\"BLEU Score:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
