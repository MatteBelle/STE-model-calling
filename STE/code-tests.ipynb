{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.6354346056137057, 'precisions': [0.9545454545454546, 0.7894736842105263, 0.5625, 0.38461538461538464], 'brevity_penalty': 1.0, 'length_ratio': 1.0476190476190477, 'translation_length': 22, 'reference_length': 21}\n",
      "Error:  Either 'lang' (e.g. 'en') or 'model_type' (e.g. 'microsoft/deberta-xlarge-mnli') must be specified\n",
      "Error:  Perplexity._compute() missing 1 required positional argument: 'model_id'\n",
      "{'rouge1': 0.9079365079365079, 'rouge2': 0.735042735042735, 'rougeL': 0.9079365079365079, 'rougeLsum': 0.9079365079365079}\n",
      "Error:  invalid literal for int() with base 10: 'The sun is shining brightly in the sky'\n",
      "Error:  Predictions and/or references don't match the expected format.\n",
      "Expected format: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\n",
      "Input predictions: ['The sun is shining brightly in the sky', 'The cat is sleeping on the couch', 'The dog is running in the park'],\n",
      "Input references: [['The sun is shining in the sky', 'The cat is sleeping on the couch', 'The dog is running in the park'], ['The sun is shining in the clear blue sky', 'The cat is sleeping on the soft couch', 'The dog is running around the park'], ['The sun is shining in the clear blue sky', 'The cat is sleeping on the soft couch', 'The dog is running around the park']]\n",
      "Error:  invalid literal for int() with base 10: 'The sun is shining brightly in the sky'\n",
      "Error:  invalid literal for int() with base 10: 'The sun is shining brightly in the sky'\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "#\"predictions\": [\"The sun is shining brightly in the sky\", \"The cat is sleeping on the couch\", \"The dog is running in the park\"]\n",
    "#\"references\": [[\"The sun is shining in the sky\", \"The cat is sleeping on the couch\", \"The dog is running in the park\"], [\"The sun is shining in the clear blue sky\", \"The cat is sleeping on the soft couch\", \"The dog is running around the park\"]]\n",
    "#\"metric_name\": \"bleu\"\n",
    "predictions = [\"The sun is shining brightly in the sky\", \"The cat is sleeping on the couch\", \"The dog is running in the park\"]\n",
    "references = [[\"The sun is shining in the sky\", \"The cat is sleeping on the couch\", \"The dog is running in the park\"], [\"The sun is shining in the clear blue sky\", \"The cat is sleeping on the soft couch\", \"The dog is running around the park\"], [\"The sun is shining in the clear blue sky\", \"The cat is sleeping on the soft couch\", \"The dog is running around the park\"]]\n",
    "metrics = [  \n",
    "  \"bleu\",\n",
    "  \"bertscore\",\n",
    "  \"perplexity\",\n",
    "  \"rouge\",\n",
    "  \"accuracy\",\n",
    "  \"exact_match\",\n",
    "  \"recall\",\n",
    "  \"f1\"\n",
    "]\n",
    "for metric_name in metrics:\n",
    "    try :\n",
    "        args = {\n",
    "            \"predictions\": predictions,\n",
    "            \"references\": references\n",
    "        }\n",
    "        metric = evaluate.load(metric_name)\n",
    "        result = metric.compute(**args)\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bububu\n"
     ]
    }
   ],
   "source": [
    "    response_text = \"blablabla <|promptends|> bububu\"\n",
    "    parts = response_text.split(\"<|promptends|>\")\n",
    "    if parts:\n",
    "        response_text = parts[1].strip()\n",
    "        print(response_text)\n",
    "    else:\n",
    "        print(\"DEBUG ERROR: <|promptends|> not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970\n",
      "1988\n"
     ]
    }
   ],
   "source": [
    "print(len(\" [SYSTEM]: You are a helpful assistant.\\\n",
    "[USER]: You are an **expert** assisting in evaluation.   \\\n",
    "Your task is to **output a single, well-structured query** that a user might naturally ask when requesting a metric evaluation.  \\\n",
    "**Context:**\\\n",
    "API_name: exact_match\\\n",
    "Description: {'description': 'Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.', 'required_parameters': [{'name': 'predictions', 'type': 'LIST of STRING', 'description': 'TList of predicted texts.', 'default': ''}, {'name': 'references', 'type': 'LIST of STRING', 'description': 'List of reference texts.', 'default': ''}], 'optional_parameters': [{'name': 'regexes_to_ignore', 'type': 'LIST of STRING', 'description': 'Regex expressions of characters to ignore when calculating the exact matches.', 'default': 'None'}, {'name': 'ignore_case', 'type': 'BOOLEAN', 'description': 'If True, turns everything to lowercase so that capitalization differences are ignored.', 'default': 'False'}, {'name': 'ignore_numbers (bool)', 'type': 'BOOLEAN', 'description': 'If True, removes all digits before comparing strings', 'default': 'False'}, {'name': 'ignore_punctuation (bool)', 'type': 'BOOLEAN', 'description': 'If True, removes punctuation before comparing strings.', 'default': 'False'}], 'example': {'predictions': ['Paris', 'London', 'Berlin'], 'references': ['Paris', 'London', 'Rome']}}\\\n",
    "\\\n",
    "**Task Instructions:**  \\\n",
    "Generate **one** realistic user query.  \\\n",
    "The query should be **concise, natural, and human-like**.  \\\n",
    "The query should **only** request metric evaluation **for a set references and predictions**.  \\\n",
    "It should provide parameters.   \\\n",
    "The query should provide very creative, diverse and long references and predictions.   \\\n",
    "Do **not** add explanations, descriptions, or metadata.  \\\n",
    "Do **not** repeat yourself.  \\\n",
    "Do **not** format the query as JSON or a code block.  \\\n",
    "**Stop after outputting the query.**\\\n",
    "\\\n",
    "User Query:\\\n",
    "\"))\n",
    "\n",
    "print(len(\"[SYSTEM]: You are a helpful assistant.\\n[USER]: You are an **expert** assisting in evaluation.   \\nYour task is to **output a single, well-structured query** that a user might naturally ask when requesting a metric evaluation.  \\n**Context:**\\nAPI_name: exact_match\\nDescription: {'description': 'Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.', 'required_parameters': [{'name': 'predictions', 'type': 'LIST of STRING', 'description': 'TList of predicted texts.', 'default': ''}, {'name': 'references', 'type': 'LIST of STRING', 'description': 'List of reference texts.', 'default': ''}], 'optional_parameters': [{'name': 'regexes_to_ignore', 'type': 'LIST of STRING', 'description': 'Regex expressions of characters to ignore when calculating the exact matches.', 'default': 'None'}, {'name': 'ignore_case', 'type': 'BOOLEAN', 'description': 'If True, turns everything to lowercase so that capitalization differences are ignored.', 'default': 'False'}, {'name': 'ignore_numbers (bool)', 'type': 'BOOLEAN', 'description': 'If True, removes all digits before comparing strings', 'default': 'False'}, {'name': 'ignore_punctuation (bool)', 'type': 'BOOLEAN', 'description': 'If True, removes punctuation before comparing strings.', 'default': 'False'}], 'example': {'predictions': ['Paris', 'London', 'Berlin'], 'references': ['Paris', 'London', 'Rome']}}\\n\\n**Task Instructions:**  \\nGenerate **one** realistic user query.  \\nThe query should be **concise, natural, and human-like**.  \\nThe query should **only** request metric evaluation **for a set references and predictions**.  \\nIt should provide parameters.   \\nThe query should provide very creative, diverse and long references and predictions.   \\nDo **not** add explanations, descriptions, or metadata.  \\nDo **not** repeat yourself.  \\nDo **not** format the query as JSON or a code block.  \\n**Stop after outputting the query.**\\n\\nUser Query:\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected subgroup: basic_classification\n",
      "Metrics in subgroup: ['accuracy', 'f1']\n",
      "Optional parameter flags:\n",
      "  accuracy: False\n",
      "  f1: True\n"
     ]
    }
   ],
   "source": [
    "# Code to generate a random metric subgroup with optional flags for each metric.\n",
    "\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "def get_random_metric_subgroup_with_flags(json_path=\"tool_metadata/API_subgroups.json\"):\n",
    "    \"\"\"\n",
    "    Loads the metric subgroups from a JSON file, randomly selects one subgroup,\n",
    "    and for each metric in that subgroup, generates a boolean flag that is True\n",
    "    with 30% probability (and False with 70% probability).\n",
    "\n",
    "    Parameters:\n",
    "        json_path (str): Path to the JSON file containing the metric subgroups.\n",
    "\n",
    "    Returns:\n",
    "        parameters: A parametersionary with the following keys:\n",
    "            - \"name\": The name of the selected subgroup.\n",
    "            - \"metrics\": A list of metric names in the subgroup.\n",
    "            - \"optional_flags\": A parametersionary mapping each metric name to a boolean flag.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"Subgroups JSON file not found at: {json_path}\")\n",
    "    \n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        subgroups = json.load(f)\n",
    "    \n",
    "    # Randomly choose one subgroup from the parametersionary values.\n",
    "    chosen_subgroup = random.choice(list(subgroups.values()))\n",
    "    \n",
    "    # For each metric in the subgroup, assign a boolean flag (True with probability 0.3).\n",
    "    optional_flags = {metric: (random.random() < 0.3) for metric in chosen_subgroup[\"metrics\"]}\n",
    "    \n",
    "    return {\n",
    "        \"name\": chosen_subgroup[\"name\"],\n",
    "        \"metrics\": chosen_subgroup[\"metrics\"],\n",
    "        \"optional_flags\": optional_flags\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "#if __name__ == \"__main__\":\n",
    "subgroup_with_flags = get_random_metric_subgroup_with_flags()\n",
    "print(\"Selected subgroup:\", subgroup_with_flags[\"name\"])\n",
    "print(\"Metrics in subgroup:\", subgroup_with_flags[\"metrics\"])\n",
    "print(\"Optional parameter flags:\")\n",
    "for metric, flag in subgroup_with_flags[\"optional_flags\"].items():\n",
    "    print(f\"  {metric}: {flag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: nltk in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: absl-py in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (2.1.0)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: torch in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (4.48.1)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (1.15.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (1.6.1)\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: dill in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (0.28.1)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: click in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from rouge-score) (1.17.0)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: matplotlib in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from bert_score) (3.10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (19.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from matplotlib->bert_score) (4.55.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from matplotlib->bert_score) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from matplotlib->bert_score) (3.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/envs/LLM/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=724072b1a4b5855afb55be76c4004fd54fc018c77e5bf782d26acb06ddf05e05\n",
      "  Stored in directory: /Users/a39328/Library/Caches/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: tabulate, portalocker, colorama, sacrebleu, rouge-score, bert_score\n",
      "Successfully installed bert_score-0.3.13 colorama-0.4.6 portalocker-3.1.1 rouge-score-0.1.2 sacrebleu-2.5.1 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate nltk absl-py rouge-score sacrebleu torch transformers numpy scipy scikit-learn bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating metric: rouge\n",
      "Results for rouge: {'rouge1': 0.5, 'rouge2': 0.33333333333333326, 'rougeL': 0.5, 'rougeLsum': 0.5}\n",
      "Evaluating metric: bleu\n",
      "Results for bleu: {'bleu': 0.3181877033696365, 'precisions': [0.6363636363636364, 0.4, 0.3333333333333333, 0.25], 'brevity_penalty': 0.8337529180751805, 'length_ratio': 0.8461538461538461, 'translation_length': 11, 'reference_length': 13}\n",
      "Evaluating metric: bertscore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bertscore: {'precision': [0.9666957259178162], 'recall': [0.9491630792617798], 'f1': [0.9578492045402527], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.48.1)'}\n",
      "Evaluating metric: perplexity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575dfe340dcc46b7a66e98f7bae3a237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for perplexity: {'perplexities': [14.477904319763184], 'mean_perplexity': 14.477904319763184}\n",
      "Evaluating metric: accuracy\n",
      "Results for accuracy: {'accuracy': 0.75}\n",
      "Evaluating metric: exact_match\n",
      "Results for exact_match: {'exact_match': 1.0}\n",
      "Evaluating metric: recall\n",
      "Results for recall: {'recall': 1.0}\n",
      "Evaluating metric: r_squared\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bf35ff7dd54f369227d9fae8c5788a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating r_squared: r_squared._compute() got an unexpected keyword argument 'squared'\n",
      "Evaluating metric: f1\n",
      "Results for f1: {'f1': 0.8}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load the API metrics from the JSON file.\n",
    "api_metrics_json_path = \"tool_metadata/API_list.json\"\n",
    "if not os.path.exists(api_metrics_json_path):\n",
    "    raise FileNotFoundError(f\"API metrics JSON file not found at: {api_metrics_json_path}\")\n",
    "\n",
    "with open(api_metrics_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    api_metrics = json.load(f)\n",
    "\n",
    "# Dictionary specifying the input format for each metric\n",
    "metric_inputs = {\n",
    "    \"rouge\": {\"predictions\": [\"Once upon a time, there was a brave hero.\"], \n",
    "              \"references\": [\"Once upon a time, a fearless warrior embarked on a journey.\"]},\n",
    "    \"bleu\": {\"predictions\": [\"Once upon a time, there was a brave hero.\"], \n",
    "             \"references\": [[\"Once upon a time, a fearless warrior embarked on a journey.\"]]},\n",
    "    \"bertscore\": {\"predictions\": [\"Once upon a time, there was a brave hero.\"], \n",
    "                  \"references\": [\"Once upon a time, a fearless warrior embarked on a journey.\"], \"lang\": \"en\"},\n",
    "    \"perplexity\": {\"predictions\": [\"Once upon a time, there was a brave hero.\"], \"model_id\": \"gpt2\"},\n",
    "    \"accuracy\": {\"predictions\": [1, 0, 1, 1], \"references\": [1, 0, 1, 0]},\n",
    "    \"exact_match\": {\"predictions\": [\"Once upon a time, there was a brave hero.\"], \n",
    "                    \"references\": [\"Once upon a time, there was a brave hero.\"]},\n",
    "    \"recall\": {\n",
    "        \"predictions\": [1, 0, 1, 1], \n",
    "        \"references\": [1, 0, 1, 0]  # Changed to numeric classification labels\n",
    "    },\n",
    "    \"f1\": {\n",
    "        \"predictions\": [1, 0, 1, 1], \n",
    "        \"references\": [1, 0, 1, 0]  # Changed to numeric classification labels\n",
    "    }\n",
    "}\n",
    "\n",
    "# Evaluate each metric in the API list.\n",
    "for metric_name in api_metrics:\n",
    "    if metric_name not in metric_inputs:\n",
    "        print(f\"Skipping {metric_name}: No input data provided.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Evaluating metric: {metric_name}\")\n",
    "\n",
    "    try:\n",
    "        # Load the metric\n",
    "        metric = evaluate.load(metric_name)\n",
    "\n",
    "        # Compute the metric\n",
    "        results = metric.compute(**metric_inputs[metric_name])\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Results for {metric_name}: {results}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {metric_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp =''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "this is a test\n"
     ]
    }
   ],
   "source": [
    "response = \"this is a test\"\n",
    "print(len(response))\n",
    "print(response[:len(response)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatch in the number of predictions (2) and references (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m references \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe cat is sitting on the mat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dog is sleeping on the bed\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Compute BLEU score\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mbleu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/LLM/lib/python3.11/site-packages/evaluate/module.py:455\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/LLM/lib/python3.11/site-packages/evaluate/module.py:546\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m     )\n\u001b[0;32m--> 546\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch in the number of predictions (2) and references (1)"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load the BLEU metric\n",
    "bleu = load(\"bleu\")\n",
    "# Example predictions and references\n",
    "predictions = [\"The cat is sitting on the mat\", \"The dog is sleeping on the bed\"]\n",
    "\n",
    "references = [[\"The cat is sitting on the mat\", \"The dog is sleeping on the bed\"]]\n",
    "\n",
    "# Compute BLEU score\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Display results\n",
    "print(\"BLEU Score:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
