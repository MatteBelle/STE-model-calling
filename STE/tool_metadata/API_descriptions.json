{
  "rouge": "{\"description\": \"ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.\", \"required_parameters\": [{\"name\": \"predictions\", \"type\": \"LIST of STRING\", \"description\": \"A list of generated texts (e.g., summaries or translations) with tokens separated by spaces.\", \"default\": \"\"}, {\"name\": \"references\", \"type\": \"LIST of STRING or LIST of LIST of STRING\", \"description\": \"A list of reference texts, or a list of reference lists if multiple references are available per prediction.\", \"default\": \"\"}], \"optional_parameters\": [{\"name\": \"rouge_types\", \"type\": \"LIST of STRING\", \"description\": \"Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).\", \"default\": \"['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\"}, {\"name\": \"use_aggregator\", \"type\": \"BOOLEAN\", \"description\": \"If True, returns aggregated scores; if False, returns individual scores for each prediction-reference pair. Defaults to True.\", \"default\": \"true\"}, {\"name\": \"use_stemmer\", \"type\": \"BOOLEAN\", \"description\": \"If True, applies a Porter stemmer to normalize words before comparison. Defaults to False.\", \"default\": \"false\"}] }",
  
  "bleu": "{\"description\": \"BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of machine translation by comparing a candidate translation against one or more reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.\", \"required_parameters\": [{\"name\": \"predictions\", \"type\": \"LIST of STRING\", \"description\": \"A list of candidate translations (each as a string).\", \"default\": \"\"}, {\"name\": \"references\", \"type\": \"LIST of LIST of STRING\", \"description\": \"A list where each element is a list of reference translations for the corresponding prediction.\", \"default\": \"\"}], \"optional_parameters\": [{\"name\": \"max_order\", \"type\": \"NUMBER\", \"description\": \"Maximum n-gram order to consider (default: 4).\", \"default\": \"4\"}, {\"name\": \"smooth\", \"type\": \"BOOLEAN\", \"description\": \"Whether to apply smoothing (default: false).\", \"default\": \"false\"}] }",
  
  "bertscore": "{\"description\": \"BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.\", \"required_parameters\": [{\"name\": \"predictions\", \"type\": \"LIST of STRING\", \"description\": \"The generated texts.\", \"default\": \"\"}, {\"name\": \"references\", \"type\": \"LIST of STRING\", \"description\": \"The reference texts.\", \"default\": \"\"}], \"optional_parameters\": [{\"name\": \"idf\", \"type\": \"BOOLEAN\", \"description\": \"Whether to use inverse document frequency reweighting (default: false).\", \"default\": \"false\"}] }",
  
  "bartscore": "{\"description\": \"BARTScore evaluates the quality of generated text by computing the conditional likelihood of the candidate given the reference using a BART model.\", \"required_parameters\": [{\"name\": \"predictions\", \"type\": \"LIST of STRING\", \"description\": \"The generated texts.\", \"default\": \"\"}, {\"name\": \"references\", \"type\": \"LIST of STRING\", \"description\": \"The reference texts.\", \"default\": \"\"}], \"optional_parameters\": [] }",
  
  "perplexity": "{\"description\": \"Perplexity measures the uncertainty of a language model when predicting a sequence of tokens. Lower perplexity indicates a better model fit.\", \"required_parameters\": [{\"name\": \"texts\", \"type\": \"LIST of STRING\", \"description\": \"A list of texts on which to compute the perplexity.\", \"default\": \"\"}], \"optional_parameters\": [] }",
  
  "accuracy": "{\"description\": \"Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.\", \"required_parameters\": [{\"name\": \"predictions\", \"type\": \"LIST of ANY\", \"description\": \"The predicted labels or outputs.\", \"default\": \"\"}, {\"name\": \"references\", \"type\": \"LIST of ANY\", \"description\": \"The ground truth labels.\", \"default\": \"\"}], \"optional_parameters\": [] }",
  
  "exact_match": "{\"description\": \"Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.\", \"required_parameters\": [{\"name\": \"predictions\", \"type\": \"LIST of STRING\", \"description\": \"The predicted answers.\", \"default\": \"\"}, {\"name\": \"references\", \"type\": \"LIST of STRING\", \"description\": \"The ground truth answers.\", \"default\": \"\"}], \"optional_parameters\": [] }",
  
  "recall": "{\"description\": \"Recall measures the proportion of actual positive instances that are correctly identified, indicating the model's sensitivity.\", \"required_parameters\": [{\"name\": \"predictions\", \"type\": \"LIST of ANY\", \"description\": \"The predicted labels.\", \"default\": \"\"}, {\"name\": \"references\", \"type\": \"LIST of ANY\", \"description\": \"The true labels.\", \"default\": \"\"}], \"optional_parameters\": [{\"name\": \"average\", \"type\": \"STRING\", \"description\": \"The averaging method to use (e.g., 'macro', 'micro', 'weighted').\", \"default\": \"macro\"}] }",
  
  "mse": "{\"description\": \"Mean Squared Error (MSE) calculates the average squared difference between predicted and true values, and is widely used to evaluate regression models.\", \"required_parameters\": [{\"name\": \"predictions\", \"type\": \"LIST of NUMBER\", \"description\": \"The predicted numeric values.\", \"default\": \"\"}, {\"name\": \"references\", \"type\": \"LIST of NUMBER\", \"description\": \"The ground truth numeric values.\", \"default\": \"\"}], \"optional_parameters\": [] }",
  
  "f1": "{\"description\": \"The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.\", \"required_parameters\": [{\"name\": \"predictions\", \"type\": \"LIST of ANY\", \"description\": \"The predicted labels.\", \"default\": \"\"}, {\"name\": \"references\", \"type\": \"LIST of ANY\", \"description\": \"The true labels.\", \"default\": \"\"}], \"optional_parameters\": [{\"name\": \"average\", \"type\": \"STRING\", \"description\": \"The averaging method to use (e.g., 'macro', 'micro', 'weighted').\", \"default\": \"macro\"}] }"
}