{
    "rouge": {
        "description": "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a suite of metrics for evaluating automatic summarization and machine translation. It measures the overlap of n-grams, word sequences, and longest common subsequences between a generated text and one or more reference texts in a case-insensitive manner.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of STRING",
                "description": "list of *N* predictions to score. Each prediction should be a string with tokens separated by spaces",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of STRING or LIST of LIST of STRING",
                "description": "list of *N* references (one for prediction) or a list of *N* lists of references. Each reference should be a string with tokens separated by spaces.",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "rouge_types",
                "type": "LIST of STRING",
                "description": "Types of ROUGE scores to compute. Defaults to ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']. Valid types include: 'rouge1' (unigram), 'rouge2' (bigram), 'rougeL' (longest common subsequence), and 'rougeLsum' (LCS on concatenated summaries).",
                "default": "['rouge1', 'rouge2', 'rougeL', 'rougeLsum']"
            },
            {
                "name": "use_aggregator",
                "type": "BOOLEAN",
                "description": "If True, returns aggregated scores; if False, returns individual scores for each prediction-reference pair. Defaults to True.",
                "default": "true"
            },
            {
                "name": "use_stemmer",
                "type": "BOOLEAN",
                "description": "If True, applies a Porter stemmer to normalize words before comparison. Defaults to False.",
                "default": "false"
            }
        ],
        "example": {
            "predictions": [
                "the cat sat on the mat",
                "the quick brown fox"
            ],
            "references": [
                "the cat sat on the mat",
                "the quick brown fox jumps over the lazy dog"
            ]
        }
    },
    "bleu": {
        "description": "BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of *N* machine translation by comparing a candidate translation against *N* reference translations. It computes the geometric mean of n-gram precisions with a brevity penalty to account for overly short translations. BLEU scores range from 0 to 1, with higher scores indicating closer similarity to human translations.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of STRING",
                "description": "List of *N* translations to score",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of STRING or LIST of LIST of STRING",
                "description": "list of *N* reference translations or a list of *N* lists of reference translations.",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "max_order",
                "type": "NUMBER",
                "description": "Maximum n-gram order to consider (default: 4).",
                "default": "4"
            },
            {
                "name": "smooth",
                "type": "BOOLEAN",
                "description": "Whether to apply smoothing (default: false).",
                "default": "false"
            }
        ],
        "example": {
            "predictions": [
                "the cat sat on the mat",
                "a quick brown fox"
            ],
            "references": [
                [
                    "the cat is sitting on the mat"
                ],
                [
                    "a fast brown fox jumps over the lazy dog"
                ]
            ]
        }
    },
    "bertscore": {
        "description": "BERTScore uses contextual embeddings from a BERT model to evaluate the similarity between candidate and reference texts by computing cosine similarity over tokens.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of STRING",
                "description": "The generated sentences.",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of STRING",
                "description": "The reference sentences.",
                "default": ""
            },
            {
                "name": "lang",
                "type": "STRING",
                "description": "The language of the input text (e.g., 'en' for English).",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "idf",
                "type": "BOOLEAN",
                "description": "Whether to use inverse document frequency reweighting (default: false).",
                "default": "false"
            }
        ],
        "example": {
            "predictions": [
                "The quick brown fox",
                "jumps over the lazy dog"
            ],
            "references": [
                "The quick brown fox",
                "leaps over the lazy dog"
            ],
            "lang": "en"
        }
    },
    "perplexity": {
        "description": "Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of STRING",
                "description": "input text, where each separate text snippet is one list entry.",
                "default": ""
            },
            {
                "name": "model_id",
                "type": "STRING",
                "description": "model_id (str): model used for calculating Perplexity. NOTE: Perplexity can only be calculated for causal language models, such as gpt2, causal variations of bert, causal versions of t5. (e.g., 'gpt2').",
                "default": ""
            }
        ],
        "optional_parameters": [
          {
              "name": "batch_size",
              "type": "INT",
              "description": "the batch size to run texts through the model.",
              "default": "16"
          },
          {
              "name": "add_start_token",
              "type": "BOOLEAN",
              "description": "whether to add the start token to the texts, so the perplexity can include the probability of the first word.",
              "default": "True"
          }
      ],
        "example": {
            "texts": [
                "The quick brown fox jumps over the lazy dog.",
                "An apple a day keeps the doctor away."
            ],
            "model_id": "gpt2"
        }
    },
    "accuracy": {
        "description": "Accuracy computes the proportion of correct predictions among all predictions, and is widely used for classification tasks.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of INT",
                "description": "Predicted labels",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of INT",
                "description": "Ground truth labels.",
                "default": ""
            }
        ],
        "optional_parameters": [
          {
            "name": "normalize",
            "type": "BOOLEAN",
            "description": "If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples",
            "default": "True"
          },
          {
            "name": "sample_weight",
            "type": "LIST of FLOAT",
            "description": "Predicted labels",
            "default": "None"
          }
        ],
        "example": {
            "predictions": [
                1,
                0,
                1,
                1
            ],
            "references": [
                1,
                1,
                1,
                0
            ]
        }
    },
    "exact_match": {
        "description": "Exact Match computes the percentage of predictions that exactly match the reference answers, a common metric in question answering and similar tasks.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of STRING",
                "description": "TList of predicted texts.",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of STRING",
                "description": "List of reference texts.",
                "default": ""
            }
        ],
        "optional_parameters": [
          {
              "name": "regexes_to_ignore",
              "type": "LIST of STRING",
              "description": "Regex expressions of characters to ignore when calculating the exact matches.",
              "default": "None"
          },
          {
              "name": "ignore_case",
              "type": "BOOLEAN",
              "description": "If True, turns everything to lowercase so that capitalization differences are ignored.",
              "default": "False"
          },
          {
              "name": "ignore_numbers (bool)",
              "type": "BOOLEAN",
              "description": "If True, removes all digits before comparing strings",
              "default": "False"
          },
          {
              "name": "ignore_punctuation (bool)",
              "type": "BOOLEAN",
              "description": "If True, removes punctuation before comparing strings.",
              "default": "False"
          }
      ],
        "example": {
            "predictions": [
                "Paris",
                "London",
                "Berlin"
            ],
            "references": [
                "Paris",
                "London",
                "Rome"
            ]
        }
    },
    "recall": {
        "description": "Recall measures the proportion of actual positive instances that are correctly identified (Recall = TP / (TP + FN)), indicating the model's sensitivity.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of INT",
                "description": "Predicted labels.",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of INT",
                "description": "Ground Truth labels.",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "average",
                "type": "STRING",
                "description": "This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data (e.g., 'binary', 'macro', 'micro', 'weighted', 'samples').",
                "default": "binary"
            },
            {
                "name": "labels",
                "type": "LIST of INT",
                "description": "The set of labels to include when average is not set to 'binary', and the order of the labels if average is None.",
                "default": "None"
            },
            {
                "name": "pos_label",
                "type": "INT",
                "description": "The class to be considered the positive class, in the case where average is set to binary.",
                "default": "1"
            },
            {
                "name": "sample_weight",
                "type": "LIST of FLOAT",
                "description": "Sample weights.",
                "default": "None"
            },
            {
                "name": "sample_weight",
                "type": "LIST of FLOAT",
                "description": "Sample weights.",
                "default": "None"
            }
        ],
        "example": {
            "predictions": [
                1,
                0,
                1,
                0
            ],
            "references": [
                1,
                1,
                1,
                0
            ]
        }
    },
    "f1": {
        "description": "The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a classifier's performance, especially when classes are imbalanced.",
        "required_parameters": [
            {
                "name": "predictions",
                "type": "LIST of ANY",
                "description": "The *N* predicted labels.",
                "default": ""
            },
            {
                "name": "references",
                "type": "LIST of ANY",
                "description": "The *N* true labels.",
                "default": ""
            }
        ],
        "optional_parameters": [
            {
                "name": "average",
                "type": "STRING",
                "description": "The averaging method to use (e.g., 'macro', 'micro', 'weighted').",
                "default": "macro"
            }
        ],
        "example": {
            "predictions": [
                1,
                0,
                1,
                0
            ],
            "references": [
                1,
                1,
                1,
                0
            ]
        }
    }
  }