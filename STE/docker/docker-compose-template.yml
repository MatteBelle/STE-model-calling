services:
  llm-server-JOBID:
    build:
      context: .
      dockerfile: Dockerfile.server
    container_name: llm-server-JOBID
    network_mode: "host"  # Use host network mode
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ['GPUID']
    environment:
      - HF_HOME=/huggingface_cache
      - HUGGING_FACE_HUB_TOKEN=<YOUR_HUGGING_FACE_TOKEN>
      - SERVER_PORT=SERVERPORT
    volumes:
      - /home/belletti/huggingface_cache:/huggingface_cache
      - /home/belletti/STE-model-calling/STE:/home/belletti/STE-model-calling/STE
    # Fix: Use the environment variable in the command
    command: ["python3", "-m", "uvicorn", "llm_server:app", "--host", "0.0.0.0", "--port", "SERVERPORT"]

  llm-main-JOBID:
    build:
      context: .
      dockerfile: Dockerfile.main
    container_name: llm-main-JOBID
    network_mode: "host"  # Use host network mode
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ['GPUID']
    environment:
      - HF_HOME=/huggingface_cache
      - HUGGING_FACE_HUB_TOKEN=<YOUR_HUGGING_FACE_TOKEN>
      - NUM_EPISODES=40
      - NUM_STM_SLOTS=4
      - MAX_TURN=4
      - MODEL_SERVER_URL=http://localhost:SERVERPORT/generate
    volumes:
      - /home/belletti/STE-model-calling/STE:/home/belletti/STE-model-calling/STE
      - /home/belletti/huggingface_cache:/huggingface_cache
    depends_on:
      - llm-server-JOBID